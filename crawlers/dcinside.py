import os
import requests
from bs4 import BeautifulSoup
import time
import random
import json
import logging
import hashlib
from urllib.parse import quote, urljoin, parse_qs, urlparse
from datetime import datetime
from selenium import webdriver

from core.base_crawler import BaseCrawler
from utils.savers import DataSaver
from core.sentiment_analysis_ensemble import EnsembleSentimentAnalyzer

class DCInsideCrawler(BaseCrawler):
    """ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ëŸ¬ - ê°¤ëŸ¬ë¦¬ ê²Œì‹œê¸€ ë° ëŒ“ê¸€ ìˆ˜ì§‘"""
    
    def __init__(self, keywords, max_pages=5, max_comments=30, save_dir="data/raw", analyze_sentiment=True, respect_robots=True, browser_type="chrome"):
        """
        DCInside í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            max_comments: ê° ê²Œì‹œê¸€ë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜
            save_dir: ì €ì¥ ë””ë ‰í„°ë¦¬
            analyze_sentiment: ê°ì„± ë¶„ì„ ìˆ˜í–‰ ì—¬ë¶€
            respect_robots: robots.txt ì •ì±… ì¤€ìˆ˜ ì—¬ë¶€ (True: ì¤€ìˆ˜, False: ë¬´ì‹œ)
            browser_type: ì‚¬ìš©í•  ë¸Œë¼ìš°ì € íƒ€ì… ("chrome" ë˜ëŠ” "firefox")
        """
        super().__init__(keywords, max_pages, save_dir)
        self.base_url = "https://search.dcinside.com/post/p"
        self.post_base_url = "https://gall.dcinside.com"
        self.max_comments = max_comments
        self.doc_ids = set()
        self.analyze_sentiment = analyze_sentiment
        self.respect_robots = respect_robots
        self.browser_type = browser_type
        
        # í•„í„°ë§ ì¡°ê±´
        self.filter_conditions = {
            'min_content_length': 50,
            'max_pages': max_pages,
            'min_confidence': 0.0,
            'exclude_keywords': ['ê´‘ê³ ', 'í™ë³´', 'sponsored', 'ì¶œì²˜', 'ì €ì‘ê¶Œ'],
            'required_keywords': ['ë¶€ì•ˆ'],
            'date_range': None,
            'include_notice': True
        }
        
        # HTTP í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.dcinside.com/'
        }
        
        # Selenium ì˜µì…˜ ì„¤ì •
        if browser_type == "chrome":
            from selenium.webdriver.chrome.options import Options
            options = Options()
        elif browser_type == "firefox":
            from selenium.webdriver.firefox.options import Options
            options = Options()
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì € íƒ€ì…: {browser_type}")
            
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-notifications")
        
        self.options = options
        
        # ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self.sentiment_analyzer = None
        
        # ë¡œê¹… ì„¤ì •
        self.logger.setLevel(logging.INFO)
        
        # í‚¤ì›Œë“œ ì„¤ì •
        if isinstance(keywords, list) and all(isinstance(k, dict) for k in keywords):
            self.original_keywords = keywords
            self.keywords = [k['text'] for k in keywords]
        else:
            self.keywords = keywords if isinstance(keywords, list) else [keywords]
            self.original_keywords = [{'text': k, 'condition': 'AND'} for k in self.keywords]
            
        # robots.txt ì •ì±… í™•ì¸ ë° ê²½ê³ 
        if self.respect_robots:
            self.logger.warning("""
            âš ï¸ DCinside robots.txt ì£¼ì˜ì‚¬í•­ âš ï¸
            DCinsideì˜ robots.txtëŠ” ì¼ë°˜ í¬ë¡¤ëŸ¬ì˜ ì ‘ê·¼ì„ ì „ì²´ì ìœ¼ë¡œ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤(User-agent: * / Disallow: /).
            ê²€ìƒ‰ ì—”ì§„ ë´‡(Googlebot, Yeti ë“±)ë§Œ ì ‘ê·¼ì„ í—ˆìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
            ì´ í¬ë¡¤ëŸ¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ robots.txt ì •ì±…ì„ ì¤€ìˆ˜í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆì–´ì„œ ì‘ë™í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
            í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ë ¤ë©´ respect_robots=Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.
            ì›¹ì‚¬ì´íŠ¸ ì •ì±…ì„ ìœ„ë°˜í•˜ë©´ ë²•ì  ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.
            """)
    
    def _search_posts(self, keyword, page=1):
        """ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ê²Œì‹œê¸€ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ URL ìƒì„±
            encoded_keyword = quote(keyword)
            search_url = f"{self.base_url}?q={encoded_keyword}&p={page}"
            
            # ìš”ì²­
            response = requests.get(
                search_url,
                headers=self.headers,
                timeout=10
            )
            
            # ğŸ” Save HTML for debugging
            debug_path = os.path.join("debug", f"dcinside_search_{keyword}_{page}.html")
            os.makedirs(os.path.dirname(debug_path), exist_ok=True)
            with open(debug_path, "w", encoding="utf-8") as f:
                f.write(response.text)
            self.logger.info(f"ğŸ” ê²€ìƒ‰ HTML ì €ì¥ë¨: {debug_path}")
            
            # ì‘ë‹µ í™•ì¸
            if response.status_code != 200:
                self.logger.error(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return []
                
            # íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ í™•ì¸
            no_result = soup.select_one('.search_no_data')
            if no_result:
                self.logger.info(f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {keyword}")
                return []
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (ìµœì‹  ì„ íƒìë¡œ ë³€ê²½)
            posts = []
            post_items = soup.select('ul.sch_result_list > li')
            
            if not post_items:
                self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {keyword} (í˜ì´ì§€ {page})")
                return []
            
            self.logger.info(f"ê²€ìƒ‰ ê²°ê³¼ {len(post_items)}ê°œ ë°œê²¬")
            
            for item in post_items:
                try:
                    # ì œëª© ì¶”ì¶œ (ìµœì‹  ì„ íƒìë¡œ ë³€ê²½)
                    title_el = item.select_one('a.sch_tit')
                    if not title_el:
                        continue
                        
                    title = self._clean_text(title_el.get_text())
                    
                    # URL ì¶”ì¶œ (ì´ë¯¸ ì ˆëŒ€ ê²½ë¡œ)
                    post_url = title_el.get('href')
                    if not post_url:
                        continue
                        
                    # ì‘ì„±ì ì¶”ì¶œ (ìµœì‹  ì„ íƒìë¡œ ë³€ê²½)
                    author_el = item.select_one('.user_nick')
                    author = self._clean_text(author_el.get_text()) if author_el else "Unknown"
                    
                    # ì‘ì„±ì¼ ì¶”ì¶œ (ìµœì‹  ì„ íƒìë¡œ ë³€ê²½)
                    date_el = item.select_one('.date')
                    pub_date = self._clean_text(date_el.get_text()) if date_el else None
                    
                    if title and post_url:
                        posts.append({
                            'title': title,
                            'url': post_url,
                            'published_date': pub_date,
                            'author': author
                        })
                        self.logger.debug(f"ê²Œì‹œê¸€ íŒŒì‹± ì„±ê³µ: {title}")
                except Exception as e:
                    self.logger.error(f"ê²Œì‹œê¸€ í•­ëª© íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
                    
            return posts
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def _get_post_content(self, post_url):
        """ê²Œì‹œê¸€ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(post_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª©
            title_el = soup.select_one('.title_subject')
            title = self._clean_text(title_el.get_text()) if title_el else ""
            
            # ë³¸ë¬¸
            content_el = soup.select_one('div.write_div')
            content = self._clean_text(content_el.get_text()) if content_el else ""
            
            # ì‘ì„±ì
            author_el = soup.select_one('span.nickname')
            author = self._clean_text(author_el.get_text()) if author_el else "Unknown"
            
            # ì‘ì„±ì¼
            date_el = soup.select_one('span.gall_date')
            pub_date = self._clean_text(date_el.get('title') or date_el.get_text()) if date_el else None
            
            # ëŒ“ê¸€
            comments = self._get_comments(post_url)
            
            return {
                'title': title,
                'content': content,
                'author': author,
                'published_date': pub_date,
                'comments': comments,
                'url': post_url
            }
            
        except Exception as e:
            self.logger.error(f"ê²Œì‹œê¸€ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def crawl(self):
        """ë””ì‹œì¸ì‚¬ì´ë“œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            start_time = time.time()
            self.logger.info(f"====== í¬ë¡¤ë§ ì‹œì‘: {time.strftime('%Y-%m-%d %H:%M:%S')} ======")
            
            all_posts = []
            
            for keyword in self.keywords:
                self.logger.info(f"\n===== í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹œì‘ =====")
                
                for page in range(1, self.filter_conditions['max_pages'] + 1):
                    self.logger.info(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                    posts = self._search_posts(keyword, page)
                    
                    if not posts:
                        break
                        
                    for post in posts:
                        try:
                            post_details = self._get_post_content(post['url'])
                            if post_details:
                                # ê²Œì‹œê¸€ ID ìƒì„±
                                doc_id = hashlib.sha256(post['url'].encode()).hexdigest()
                                
                                if doc_id in self.doc_ids:
                                    continue
                                    
                                self.doc_ids.add(doc_id)
                                
                                # ê°ì„± ë¶„ì„
                                combined_text = f"{post_details['title']} {post_details['content']}"
                                sentiment, confidence = self.analyze_text_sentiment(combined_text)
                                
                                post_data = {
                                    **post_details,
                                    'platform': 'dcinside',
                                    'keyword': keyword,
                                    'original_keywords': ",".join(self.keywords),
                                    'sentiment': sentiment,
                                    'confidence': confidence,
                                    'doc_id': doc_id,
                                    'crawled_at': time.strftime("%Y-%m-%d %H:%M:%S")
                                }
                                
                                all_posts.append(post_data)
                                
                        except Exception as e:
                            self.logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                            
                    time.sleep(random.uniform(0.5, 1.0))
                    
            # í›„ì²˜ë¦¬ ì ìš©
            filtered_posts = self._postprocess(all_posts, self.original_keywords)
            
            # ê²°ê³¼ ì €ì¥
            if filtered_posts:
                keywords_str = '_'.join(self.keywords)
                filename = f"dcinside_{len(filtered_posts)}_{keywords_str}_{time.strftime('%Y%m%d_%H%M%S')}.json"
                filepath = os.path.join(self.save_dir, filename)
                os.makedirs(self.save_dir, exist_ok=True)
                
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(filtered_posts, f, ensure_ascii=False, indent=2)
                    
                self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
                
            # í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ìš”ì•½
            end_time = time.time()
            elapsed_time = end_time - start_time
            self.logger.info(f"í¬ë¡¤ë§ ì¢…ë£Œ: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info(f"ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.2f}ë¶„)")
            self.logger.info(f"ìˆ˜ì§‘ëœ ì´ ë¬¸ì„œ: {len(filtered_posts)}ê°œ")
            
            return filtered_posts
            
        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

    def _get_comments(self, post_url):
        """ê²Œì‹œê¸€ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        comments = []
        try:
            # URLì—ì„œ ê°¤ëŸ¬ë¦¬ IDì™€ ê²Œì‹œê¸€ ë²ˆí˜¸ ì¶”ì¶œ
            parsed_url = urlparse(post_url)
            query_params = parse_qs(parsed_url.query)
            
            gallery_id = query_params.get('id', [''])[0]
            post_id = query_params.get('no', [''])[0]
            
            if not gallery_id or not post_id:
                return comments
                
            # ëŒ“ê¸€ API URL
            comment_url = f"{self.post_base_url}/board/comment/"
            
            data = {
                'id': gallery_id,
                'no': post_id,
                'cmt_id': gallery_id,
                'cmt_no': post_id,
                'e_s_n_o': '3eabc219ebdd65f1'
            }
            
            headers = self.headers.copy()
            headers.update({
                'X-Requested-With': 'XMLHttpRequest',
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                'Origin': self.post_base_url,
                'Referer': post_url
            })
            
            response = requests.post(comment_url, data=data, headers=headers, timeout=10)
            
            if response.status_code == 200:
                try:
                    comment_data = response.json()
                    comment_html = comment_data.get('comments', '')
                    comment_soup = BeautifulSoup(comment_html, 'html.parser')
                    
                    for i, comment_el in enumerate(comment_soup.select('li.ub-content')):
                        if i >= self.max_comments:
                            break
                            
                        try:
                            nick_el = comment_el.select_one('span.nickname')
                            content_el = comment_el.select_one('p.usertxt')
                            date_el = comment_el.select_one('span.date_time')
                            
                            comment = {
                                'author': self._clean_text(nick_el.get_text()) if nick_el else "Unknown",
                                'content': self._clean_text(content_el.get_text()) if content_el else "",
                                'date': self._clean_text(date_el.get_text()) if date_el else "",
                                'sentiment': None,
                                'confidence': None
                            }
                            
                            if self.analyze_sentiment:
                                sentiment, confidence = self.analyze_text_sentiment(comment['content'])
                                comment.update({
                                    'sentiment': sentiment,
                                    'confidence': confidence
                                })
                                
                            comments.append(comment)
                            
                        except Exception as e:
                            self.logger.error(f"ëŒ“ê¸€ í•­ëª© íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                            
                except json.JSONDecodeError:
                    self.logger.error("ëŒ“ê¸€ ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤.")
                    
        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        return comments
    
    def _clean_text(self, text):
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
            
        # ê¸°ë³¸ ì •ë¦¬
        text = super().clean_text(text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
        text = text.replace('&nbsp;', ' ')
        text = text.replace('\xa0', ' ')
        text = text.replace('\u200b', '')
        
        return text.strip()
    
    def _postprocess(self, items, original_keywords):
        """ìˆ˜ì§‘ëœ ì•„ì´í…œì„ í›„ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        processed_items = []
        filtered_count = 0
        
        for item in items:
            # 1. í‚¤ì›Œë“œ ì¡°ê±´ í™•ì¸
            combined_text = f"{item['title']} {item.get('content', '')}"
            
            # í•„ìˆ˜ í‚¤ì›Œë“œ(ë¶€ì•ˆ)ëŠ” ë°˜ë“œì‹œ í¬í•¨
            if not any(kw.lower() in combined_text.lower() for kw in self.filter_conditions['required_keywords']):
                filtered_count += 1
                continue
                
            # ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì œì™¸
            if any(kw.lower() in combined_text.lower() for kw in self.filter_conditions['exclude_keywords']):
                filtered_count += 1
                continue
                
            # 2. ì¤‘ë³µ ë¬¸ì„œ í™•ì¸
            doc_id = hashlib.sha256((item['url'] + item['title']).encode()).hexdigest()
            if doc_id in self.doc_ids:
                filtered_count += 1
                continue
            
            # 3. ë‚ ì§œ ë²”ìœ„ í™•ì¸
            try:
                pub_date = datetime.strptime(item['published_date'], '%Y%m%d')
                if (datetime.now() - pub_date).days > self.filter_conditions['date_range']:
                    filtered_count += 1
                    continue
            except:
                pass
            
            # ë¬¸ì„œ ID ì¶”ê°€
            self.doc_ids.add(doc_id)
            item['doc_id'] = doc_id
            
            processed_items.append(item)
        
        # í•„í„°ë§ ë¹„ìœ¨ ê³„ì‚°
        if items:
            filter_ratio = (filtered_count / len(items)) * 100
            self.logger.info(f"í•„í„°ë§ ë¹„ìœ¨: {filter_ratio:.1f}% ({filtered_count}/{len(items)})")
        
        return processed_items
        
    def analyze_text_sentiment(self, text):
        """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„"""
        if not self.analyze_sentiment or not text:
            return None, None
            
        try:
            if self.sentiment_analyzer is None:
                self.sentiment_analyzer = EnsembleSentimentAnalyzer()
            sentiment, confidence = self.sentiment_analyzer.predict(text)
            return sentiment, confidence
        except Exception as e:
            self.logger.error(f"ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None, None 