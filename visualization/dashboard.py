import streamlit as st
import folium
from streamlit_folium import st_folium
from utils.data_loader import DataLoader
from core.sentiment_analysis import SentimentAnalyzer
from core.sentiment_analysis_kobert import KoBERTSentimentAnalyzer
from core.sentiment_analysis_ensemble import EnsembleSentimentAnalyzer
from reporting.report_generator_gpt import GPTReportGenerator
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from folium.plugins import MarkerCluster
from reporting.pdf_report_generator import PDFReportGenerator
from crawlers.naver_api_crawler import NaverSearchAPICrawler
from crawlers.youtube import YouTubeCrawler
import os
import glob
from collections import Counter
from dateutil import parser

@st.cache_data
def analyze_sentiment(text, analyzer):
    """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ê²°ê³¼ ìºì‹±"""
    sentiment, confidence = analyzer.predict(text)
    sentiment_label = 'negative' if sentiment == 0 else 'neutral' if sentiment == 1 else 'positive'
    return sentiment_label, confidence

def create_map(data):
    """Folium ì§€ë„ ìƒì„± ë° í´ëŸ¬ìŠ¤í„°ë§"""
    m = folium.Map(location=[35.728, 126.733], zoom_start=10)
    marker_cluster = MarkerCluster().add_to(m)
    
    for item in data:
        try:
            folium.Marker(
                location=[item['lat'], item['lon']],
                popup=item['content'],
                icon=folium.Icon(color='blue' if item['sentiment'] == 'positive' else 'red' if item['sentiment'] == 'negative' else 'gray')
            ).add_to(marker_cluster)
        except TypeError:
            continue
    
    return m

def plot_time_series(data):
    """ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ í”Œë¡¯"""
    df = pd.DataFrame(data)
    
    if 'date' not in df.columns:
        st.write("ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df['date'] = pd.to_datetime(df['date'])
    df.set_index('date', inplace=True)
    
    sentiment_counts = df.resample('D').sentiment.value_counts().unstack().fillna(0)
    sentiment_counts.plot(kind='line', figsize=(10, 5))
    plt.title("ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ")
    plt.xlabel("ë‚ ì§œ")
    plt.ylabel("ê°ì„± ìˆ˜")
    plt.legend(["ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •"])
    st.pyplot(plt)

def parse_user_query(query):
    """ì‚¬ìš©ì ì§ˆì˜ íŒŒì‹±"""
    filters = {}
    if "ë¶€ì •" in query:
        filters['sentiment'] = 'negative'
    if "ë³‘ì›" in query:
        filters['keyword'] = 'ë³‘ì›'
    # ì¶”ê°€ì ì¸ í•„í„°ë§ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
    return filters

def main():
    """ëŒ€ì‹œë³´ë“œ ë©”ì¸ í•¨ìˆ˜"""
    st.title("ë¶€ì•ˆêµ° ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    
    # ì‚¬ì´ë“œë°” í•„í„° UI
    st.sidebar.header("í•„í„° ì˜µì…˜")
    selected_platform = st.sidebar.selectbox("í”Œë«í¼ ì„ íƒ", ["naver_search", "youtube", "twitter"])
    selected_keyword = st.sidebar.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ë¶€ì•ˆ")
    start_date = st.sidebar.date_input("ì‹œì‘ ë‚ ì§œ", datetime(2023, 1, 1))
    end_date = st.sidebar.date_input("ì¢…ë£Œ ë‚ ì§œ", datetime.now())
    
    # JSON íŒŒì¼ ê°œìˆ˜ í™•ì¸
    json_files = glob.glob("data/raw/**/*.json", recursive=True)
    json_counts = Counter([f.split('_')[0].replace("data/raw/", "") for f in json_files])
    for platform, count in json_counts.items():
        st.sidebar.write(f"{platform}: {count}ê°œ")
    
    # ê°ì„± ë¶„ì„ê¸° ì„ íƒ
    analyzer_option = st.sidebar.selectbox("ê°ì„± ë¶„ì„ê¸° ì„ íƒ", ["Naive Bayes", "KoBERT", "Ensemble"])
    
    # âœ… ë°ì´í„° ìˆ˜ì§‘ ë²„íŠ¼ ì¶”ê°€
    if st.sidebar.button("ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"):
        if selected_platform == "naver_search":
            crawler = NaverSearchAPICrawler(
            keywords=[selected_keyword],
            start_date=start_date.strftime('%Y%m%d'),
            end_date=end_date.strftime('%Y%m%d')
            )
            crawler.crawl()
            st.success("âœ… ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ!")
        elif selected_platform == "youtube":
            crawler = YouTubeCrawler(keywords=[selected_keyword], max_results=30)
            crawler.crawl()
            st.success("âœ… ìœ íŠœë¸Œ í¬ë¡¤ë§ ì™„ë£Œ!")
        # íŠ¸ìœ„í„° ë“± ì¶”ê°€ ê°€ëŠ¥
    
    # ë°ì´í„° ë¡œë“œ
    data_loader = DataLoader()
    data = data_loader.load_data(
        platform=selected_platform, 
        keyword=selected_keyword, 
        start_date=start_date, 
        end_date=end_date
    )
    
    # ê¸°ê°„ í•„í„°ë§
    filtered_data = [
        d for d in data 
        if 'published_date' in d and start_date.strftime('%Y%m%d') <= d['published_date'] <= end_date.strftime('%Y%m%d')
    ]
    
    # ì¤‘ë³µ í•„í„°ë§
    unique_data = {item['url']: item for item in filtered_data}.values()
    
    # ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
    if analyzer_option == "Naive Bayes":
        sentiment_analyzer = SentimentAnalyzer()
    elif analyzer_option == "KoBERT":
        sentiment_analyzer = KoBERTSentimentAnalyzer()
    elif analyzer_option == "Ensemble":
        sentiment_analyzer = EnsembleSentimentAnalyzer()
    
    # ê°ì„± ë¶„ì„ ìˆ˜í–‰
    for item in unique_data:
        if 'sentiment' not in item or item['sentiment'] is None:
            item['sentiment'], item['confidence'] = analyze_sentiment(item['content'], sentiment_analyzer)

    # ê°ì„± ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
    st.write("ğŸ“‹ ê°ì„± ë¶„ì„ ìƒ˜í”Œ", list(unique_data)[:3])

    # CSV ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ìš© DataFrame ìƒì„±
    for item in unique_data:
        if 'platform' not in item:
            item['platform'] = selected_platform  # ì‚¬ì´ë“œë°” ì„ íƒê°’ í™œìš©
        if 'sentiment' not in item:
            item['sentiment'] = 'neutral'  # ê¸°ë³¸ê°’ìœ¼ë¡œ 'neutral' ì„¤ì •

    df = pd.DataFrame(unique_data)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False), file_name="sentiment_results.csv")
    
    # ê°ì„± í†µê³„ ì‹œê°í™”
    st.subheader("ê°ì„± ë¶„í¬")
    sentiments = [item['sentiment'] for item in unique_data]
    sentiment_counts = pd.Series(sentiments).value_counts()
    st.pyplot(sentiment_counts.plot.pie(autopct="%.1f%%", figsize=(5, 5)).figure)
    
    # ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ
    st.subheader("ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ")
    positive_text = " ".join([item["content"] for item in unique_data if item["sentiment"] == "positive"])
    
    if positive_text.strip():  # ê¸ì • ê°ì„± í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        wordcloud = WordCloud(font_path="NanumGothic.ttf", background_color='white').generate(positive_text)
        st.image(wordcloud.to_array())
    else:
        st.write("ê¸ì • ê°ì„±ì˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì‹œê³„ì—´ íŠ¸ë Œë“œ
    st.subheader("ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ")
    plot_time_series(unique_data)
    
    # ì§€ë„ ì‹œê°í™”
    st.subheader("ê°ì„± ë¶„í¬ ì§€ë„")
    map_ = create_map(unique_data)
    st_folium(map_, width=700, height=500)
    
    # GPT ë¦¬í¬íŠ¸ ìƒì„±
    st.subheader("ì •ì±… ì œì•ˆ ë¦¬í¬íŠ¸")
    report_generator = GPTReportGenerator(api_key="YOUR_OPENAI_API_KEY")
    report = report_generator.generate_report(df)
    st.text(report)
    
    # PDF ë¦¬í¬íŠ¸ ì €ì¥
    pdf_generator = PDFReportGenerator()
    pdf_generator.generate_pdf(report)
    st.success("PDF ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 