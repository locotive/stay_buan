import streamlit as st
import pandas as pd
import folium
from streamlit_folium import st_folium
from utils.data_loader import DataLoader
from utils.data_normalizer import load_and_normalize_data
from utils.data_processor import DataProcessor
from core.sentiment_analysis import SentimentAnalyzer
from core.sentiment_analysis_kobert import KoBERTSentimentAnalyzer
from core.sentiment_analysis_ensemble import EnsembleSentimentAnalyzer
from reporting.report_generator_gpt import GPTReportGenerator
from datetime import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from folium.plugins import MarkerCluster
from reporting.pdf_report_generator import PDFReportGenerator
from crawlers.naver_api_crawler import NaverSearchAPICrawler
from crawlers.youtube import YouTubeCrawler
from crawlers.google_search import GoogleSearchCrawler
import os
import glob
from collections import Counter
from dateutil import parser
import json
import random
import subprocess
import threading
import sys
from pathlib import Path
import re
import time
import queue
import logging
import concurrent.futures

# ë¡œê¹… ì„¤ì •
os.makedirs("data/logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("data/logs/streamlit_dashboard.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("dashboard")

# í¬ë¡¤ë§ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
def save_crawler_status(status):
    """í¬ë¡¤ë§ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        os.makedirs("data/status", exist_ok=True)
        with open("data/status/crawler_status.json", "w") as f:
            json.dump(status, f)
        logger.info("í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ë¨")
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)

def load_crawler_status():
    """íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ìƒíƒœ ì½ê¸°"""
    try:
        if os.path.exists("data/status/crawler_status.json"):
            with open("data/status/crawler_status.json", "r") as f:
                status = json.load(f)
            logger.info("í¬ë¡¤ë§ ìƒíƒœ ë¡œë“œë¨")
            return status
        else:
            return {
                'message': '',
                'progress': 0.0,
                'result': '',
                'is_running': False,
                'update_timestamp': 0,
                'command': ''
            }
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ìƒíƒœ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            'message': '',
            'progress': 0.0,
            'result': '',
            'is_running': False,
            'update_timestamp': 0,
            'command': ''
        }

# í¬ë¡¤ë§ ìƒíƒœë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
crawler_status = load_crawler_status()

# í¬ë¡¤ë§ ìŠ¤ë ˆë“œ í•¨ìˆ˜
def run_crawler(cmd):
    """í¬ë¡¤ë§ ëª…ë ¹ ì‹¤í–‰ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
    global crawler_status
    
    try:
        # ì´ˆê¸° ìƒíƒœ ì—…ë°ì´íŠ¸
        crawler_status['message'] = "í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘..."
        crawler_status['progress'] = 0.0
        crawler_status['result'] = ""
        crawler_status['is_running'] = True
        crawler_status['update_timestamp'] = time.time()
        crawler_status['command'] = cmd
        save_crawler_status(crawler_status)  # ìƒíƒœ ì €ì¥
        
        logger.info(f"í¬ë¡¤ë§ ëª…ë ¹ ì‹¤í–‰: {cmd}")
        
        with subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            bufsize=1,
            universal_newlines=True,
            shell=True
        ) as process:
            # í”Œë«í¼ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ - ëª…ë ¹ì–´ì—ì„œ í”Œë«í¼ ì •ë³´ ì¶”ì¶œ
            all_platforms = ["naver", "youtube", "google", "dcinside", "fmkorea", "buan"]
            
            # ëª…ë ¹ì–´ì—ì„œ platform íŒŒë¼ë¯¸í„° ì°¾ê¸°
            platform_param = ""
            for part in cmd.split():
                if part.startswith("--platform"):
                    platform_param = part.split("=")[1] if "=" in part else cmd.split()[cmd.split().index(part) + 1]
                    break
            
            # í”Œë«í¼ ëª©ë¡ ê²°ì •
            if platform_param == "all" or not platform_param:
                platforms = all_platforms
            else:
                platforms = platform_param.split(",")
                
            progress_per_platform = 1.0 / len(platforms)
            current_platform = None
            platform_index = 0
            
            lines = []
            
            logger.info("í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨, ì¶œë ¥ ëª¨ë‹ˆí„°ë§ ì¤‘...")
            
            for line in process.stdout:
                line = line.strip()
                lines.append(line)
                logger.info(f"í¬ë¡¤ë§ ì¶œë ¥: {line}")
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if "í¬ë¡¤ë§ ì‹œì‘" in line:
                    for platform in platforms:
                        if platform.upper() in line:
                            current_platform = platform
                            platform_index += 1
                            status_msg = f"í˜„ì¬ í”Œë«í¼: {current_platform.upper()} í¬ë¡¤ë§ ì¤‘... ({platform_index}/{len(platforms)})"
                            crawler_status['message'] = status_msg
                            crawler_status['progress'] = min(0.99, platform_index * progress_per_platform)
                            crawler_status['update_timestamp'] = time.time()
                            save_crawler_status(crawler_status)  # ìƒíƒœ ì €ì¥
                            logger.info(status_msg)
                            break
                
                # ìµœì¢… ê²°ê³¼ í™•ì¸
                if "í†µí•© ê²°ê³¼ ì €ì¥ ê²½ë¡œ" in line:
                    result_path = line.split("í†µí•© ê²°ê³¼ ì €ì¥ ê²½ë¡œ:")[1].strip()
                    crawler_status['result'] = f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_path}"
                    crawler_status['update_timestamp'] = time.time()
                    save_crawler_status(crawler_status)  # ìƒíƒœ ì €ì¥
                    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_path}")
            
            # í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            return_code = process.wait()
            logger.info(f"í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ, ë¦¬í„´ ì½”ë“œ: {return_code}")
            
            # í¬ë¡¤ë§ ì™„ë£Œ
            crawler_status['progress'] = 1.0
            crawler_status['message'] = "âœ… í¬ë¡¤ë§ ì™„ë£Œ!"
            crawler_status['is_running'] = False
            crawler_status['update_timestamp'] = time.time()
            save_crawler_status(crawler_status)  # ìƒíƒœ ì €ì¥
            logger.info("í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ")
            
            # ì „ì²´ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥
            log_filename = f"crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            os.makedirs("data/logs", exist_ok=True)
            with open(f"data/logs/{log_filename}", "w") as f:
                f.write("\n".join(lines))
            logger.info(f"í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_filename}")
    
    except Exception as e:
        error_msg = f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        crawler_status['message'] = error_msg
        crawler_status['progress'] = 1.0
        crawler_status['is_running'] = False
        crawler_status['update_timestamp'] = time.time()
        save_crawler_status(crawler_status)  # ìƒíƒœ ì €ì¥
        logger.error(error_msg, exc_info=True)
        
        # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
        error_log_filename = f"crawl_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        os.makedirs("data/logs", exist_ok=True)
        with open(f"data/logs/{error_log_filename}", "w") as f:
            f.write(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
            f.write(f"ëª…ë ¹ì–´: {cmd}\n")
            import traceback
            f.write(traceback.format_exc())
        logger.info(f"ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: {error_log_filename}")

def get_keywords_string(region_keyword, additional_keywords):
    """í‚¤ì›Œë“œ ëª©ë¡ì„ ëª…ë ¹ì¤„ ì¸ì í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    keywords = [region_keyword]
    for kw in additional_keywords:
        if kw["text"].strip():
            keywords.append(kw["text"].strip())
    return ' '.join([f'"{k}"' if ' ' in k else k for k in keywords])

# í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
def load_latest_results(num_files=5):
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
    result_files = []
    
    # í”Œë«í¼ë³„ ìµœê·¼ íŒŒì¼ ê²€ìƒ‰
    platforms = ["naver", "youtube", "google", "dcinside", "fmkorea", "buan", "combined"]
    
    for platform in platforms:
        files = glob.glob(f"data/raw/{platform}*.json")
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬
        files.sort(key=os.path.getmtime, reverse=True)
        # ìµœê·¼ íŒŒì¼ ì„ íƒ
        for file in files[:num_files]:
            if os.path.exists(file):
                try:
                    stats = os.stat(file)
                    mtime = datetime.fromtimestamp(stats.st_mtime)
                    
                    # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    filename = os.path.basename(file)
                    match = re.search(r'(\w+)_(\d+)_([^_]+)', filename)
                    
                    platform_name = ""
                    item_count = 0
                    keywords = ""
                    
                    if match:
                        platform_name = match.group(1)
                        item_count = int(match.group(2))
                        keywords = match.group(3)
                    else:
                        # ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ì—†ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ëŒ€ì²´
                        platform_name = platform
                        item_count = 0
                        keywords = filename
                    
                    result_files.append({
                        "filename": filename,
                        "platform": platform_name,
                        "keywords": keywords,
                        "items": item_count,
                        "path": file,
                        "modified": mtime.strftime("%Y-%m-%d %H:%M:%S")
                    })
                except Exception as e:
                    st.error(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
    
    # ì‹œê°„ìˆœ ì •ë ¬
    result_files.sort(key=lambda x: x["modified"], reverse=True)
    return result_files[:num_files]

def get_latest_result_stats():
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ í†µê³„"""
    try:
        result_files = glob.glob("data/raw/combined_*.json")
        if not result_files:
            return None
            
        # ê°€ì¥ ìµœê·¼ íŒŒì¼
        latest_file = max(result_files, key=os.path.getmtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # í”Œë«í¼ë³„ í•­ëª© ìˆ˜
        platform_counts = {}
        for item in data:
            platform = item.get('platform', 'unknown')
            platform_counts[platform] = platform_counts.get(platform, 0) + 1
        
        # ê°ì„±ë³„ í•­ëª© ìˆ˜
        sentiment_counts = {'positive': 0, 'neutral': 0, 'negative': 0, 'unknown': 0}
        for item in data:
            sentiment = item.get('sentiment', 'unknown')
            if sentiment == 0 or sentiment == 'negative':
                sentiment_counts['negative'] += 1
            elif sentiment == 1 or sentiment == 'neutral':
                sentiment_counts['neutral'] += 1
            elif sentiment == 2 or sentiment == 'positive':
                sentiment_counts['positive'] += 1
            else:
                sentiment_counts['unknown'] += 1
        
        # ë‚ ì§œë³„ í•­ëª© ìˆ˜
        date_counts = {}
        for item in data:
            date = item.get('published_date', '')
            if date and len(date) >= 8:  # 'YYYYMMDD' í˜•ì‹
                date = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
            else:
                date = 'unknown'
            date_counts[date] = date_counts.get(date, 0) + 1
        
        return {
            'total': len(data),
            'filename': os.path.basename(latest_file),
            'platform_counts': platform_counts,
            'sentiment_counts': sentiment_counts,
            'date_counts': date_counts,
            'path': latest_file
        }
    except Exception as e:
        st.error(f"ê²°ê³¼ í†µê³„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

@st.cache_data
def analyze_sentiment(text, analyzer):
    """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ê²°ê³¼ ìºì‹±"""
    sentiment, confidence = analyzer.predict(text)
    sentiment_label = 'negative' if sentiment == 0 else 'neutral' if sentiment == 1 else 'positive'
    return sentiment_label, confidence

def create_map(data):
    """ì§€ë„ ì‹œê°í™” ìƒì„±"""

    # None ì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì¢…ë£Œ
    if data is None:
        return None

    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data, list):
        data = pd.DataFrame(data)

    # DataFrameì´ ì•„ë‹Œ ê²½ìš° ì¢…ë£Œ
    if not isinstance(data, pd.DataFrame):
        return None

    # DataFrameì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì¢…ë£Œ
    if data.empty:
        return None

    # ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
    if 'location' in data.columns:
        locations = data['location'].dropna().unique()
        if len(locations) > 0:
            try:
                lat, lon = map(float, locations[0].split(','))
                return folium.Map(
                    location=[lat, lon],
                    zoom_start=13,
                    tiles='CartoDB positron'
                )
            except ValueError:
                pass  # location í˜•ì‹ì´ ì´ìƒí•  ê²½ìš° ë¬´ì‹œí•˜ê³  ê¸°ë³¸ ì§€ë„ ìƒì„±

    # ìœ„ì¹˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°: ë¶€ì•ˆêµ° ì¤‘ì‹¬
    return folium.Map(
        location=[35.7284, 126.7320],
        zoom_start=11,
        tiles='CartoDB positron'
    )

def get_available_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜"""
    datasets = []
    
    # ë„¤ì´ë²„ ë°ì´í„°ì…‹
    for file in glob.glob("data/raw/naver_*.json"):
        filename = os.path.basename(file)
        parts = filename.split('_')
        if len(parts) >= 4:
            count = parts[2]
            keywords = '_'.join(parts[3:-2])
            datasets.append({
                'filename': filename,
                'count': count,
                'keywords': keywords,
                'path': file,
                'platform': 'naver'
            })
    
    # ìœ íŠœë¸Œ ë°ì´í„°ì…‹
    for file in glob.glob("data/raw/youtube_*.json"):
        filename = os.path.basename(file)
        parts = filename.split('_')
        if len(parts) >= 4:
            count = parts[1]  # youtube_[count]_[keywords]_[timestamp].json
            keywords = '_'.join(parts[2:-2])
            datasets.append({
                'filename': filename,
                'count': count,
                'keywords': keywords,
                'path': file,
                'platform': 'youtube'
            })
    
    # ê¸°íƒ€ í”Œë«í¼ (í™•ì¥ì„± ê³ ë ¤)
    for platform in ['google', 'dcinside', 'fmkorea', 'buan']:
        for file in glob.glob(f"data/raw/{platform}_*.json"):
            filename = os.path.basename(file)
            parts = filename.split('_')
            if len(parts) >= 3:
                count = parts[1]
                keywords = '_'.join(parts[2:-2])
                datasets.append({
                    'filename': filename,
                    'count': count,
                    'keywords': keywords,
                    'path': file,
                    'platform': platform
                })
    
    return sorted(datasets, key=lambda x: x['count'], reverse=True)

def load_selected_dataset(filepath):
    """ì„ íƒëœ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì •ê·œí™”"""
    try:
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        data_loader = DataLoader()
        data_processor = DataProcessor()
        
        # íŒŒì¼ëª…ì—ì„œ í”Œë«í¼ ì¶”ì¶œ
        platform = os.path.basename(filepath).split('_')[0]
        
        # ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
        df = load_and_normalize_data(filepath)
        
        # ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        df = data_processor.process_data(df, platform)
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        processed_filepath = data_processor.save_processed_data(df, platform)
        
        st.success(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {processed_filepath}")
        return df
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    """ëŒ€ì‹œë³´ë“œ ë©”ì¸ í•¨ìˆ˜"""
    global crawler_status
    
    # ì‹œì‘ ì‹œ í¬ë¡¤ë§ ìƒíƒœ íŒŒì¼ì—ì„œ ë¡œë“œ
    crawler_status = load_crawler_status()
    
    st.title("ë¶€ì•ˆêµ° ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'analyzer_option' not in st.session_state:
        st.session_state.analyzer_option = "Naive Bayes"
    if 'show_results' not in st.session_state:
        st.session_state.show_results = False
    if 'region_keyword' not in st.session_state:
        st.session_state.region_keyword = "ë¶€ì•ˆ"
    if 'additional_keywords' not in st.session_state:
        st.session_state.additional_keywords = [{"text": "", "condition": "AND"}]
    if 'dashboard_mode' not in st.session_state:
        st.session_state.dashboard_mode = "í¬ë¡¤ë§"
    if 'last_update_time' not in st.session_state:
        st.session_state.last_update_time = 0
    
    # ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€ì‹œë³´ë“œ ëª¨ë“œ ì„ íƒ
    with st.sidebar:
        st.header("ëŒ€ì‹œë³´ë“œ ëª¨ë“œ")
        dashboard_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["í¬ë¡¤ë§", "ë°ì´í„° ë¶„ì„"],
            index=0 if st.session_state.dashboard_mode == "í¬ë¡¤ë§" else 1,
            help="í¬ë¡¤ë§ ëª¨ë“œ: ë‹¤ì–‘í•œ í”Œë«í¼ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ëª¨ë“œ: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
        )
        st.session_state.dashboard_mode = dashboard_mode
        
        st.markdown("---")
        
        # ëª¨ë“œì— ë”°ë¼ ì‚¬ì´ë“œë°” ë‚´ìš© ë³€ê²½
        if dashboard_mode == "í¬ë¡¤ë§":
            st.header("í¬ë¡¤ë§ ì„¤ì •")
            
            # ì§€ì—­ í‚¤ì›Œë“œ (í•„ìˆ˜)
            region_keyword = st.text_input("ì§€ì—­ í‚¤ì›Œë“œ (í•„ìˆ˜)", value=st.session_state.region_keyword, key="crawl_region_keyword")
            if region_keyword.strip():
                st.session_state.region_keyword = region_keyword
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì„¹ì…˜
            st.markdown("##### ì¶”ê°€ í‚¤ì›Œë“œ")
            
            # í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ ì¶”ê°€/ì œê±° ë²„íŠ¼
            col_a, col_b = st.columns([1, 1])
            with col_a:
                if st.button("â• í‚¤ì›Œë“œ ì¶”ê°€", use_container_width=True, key="add_keyword_btn"):
                    st.session_state.additional_keywords.append({"text": "", "condition": "AND"})
                    st.rerun()
            with col_b:
                if st.button("â– í‚¤ì›Œë“œ ì œê±°", use_container_width=True, key="remove_keyword_btn") and len(st.session_state.additional_keywords) > 0:
                    st.session_state.additional_keywords.pop()
                    st.rerun()
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œë“¤
            for i in range(len(st.session_state.additional_keywords)):
                col_a, col_b = st.columns([3, 1])
                with col_a:
                    st.session_state.additional_keywords[i]["text"] = st.text_input(
                        f"í‚¤ì›Œë“œ {i+1}",
                        value=st.session_state.additional_keywords[i]["text"],
                        key=f"crawl_keyword_{i}"
                    )
                with col_b:
                    st.session_state.additional_keywords[i]["condition"] = st.selectbox(
                        "ì¡°ê±´",
                        ["AND", "OR"],
                        index=0 if st.session_state.additional_keywords[i]["condition"] == "AND" else 1,
                        key=f"crawl_condition_{i}"
                    )
            
            # í¬ë¡¤ë§ ì˜µì…˜ ì„¹ì…˜
            st.markdown("##### í¬ë¡¤ë§ ì˜µì…˜")
            
            # í”Œë«í¼ ì„ íƒ
            platform_groups = {
                "ë„¤ì´ë²„": ["naver"],
                "ìœ íŠœë¸Œ": ["youtube"],
                "êµ¬ê¸€": ["google"],
                "ì»¤ë®¤ë‹ˆí‹°": ["dcinside", "fmkorea"],
                "ë¶€ì•ˆêµ°ì²­": ["buan"]
            }
            
            selected_groups = st.multiselect(
                "í¬ë¡¤ë§í•  í”Œë«í¼ ì„ íƒ",
                options=list(platform_groups.keys()),
                default=list(platform_groups.keys()),
                help="ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥. ëª¨ë“  í”Œë«í¼ì„ ë™ì‹œì— í¬ë¡¤ë§í•˜ê±°ë‚˜ ì›í•˜ëŠ” í”Œë«í¼ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            # ì„ íƒëœ ê·¸ë£¹ì—ì„œ í”Œë«í¼ ëª©ë¡ ì¶”ì¶œ
            platforms = []
            for group in selected_groups:
                platforms.extend(platform_groups[group])
            
            platform_option = ",".join(platforms) if platforms else "all"
            
            # í˜ì´ì§€ ìˆ˜
            pages = st.number_input("í˜ì´ì§€/ê²°ê³¼ ìˆ˜", min_value=1, max_value=100, value=3, key="crawl_pages")
            
            # ëŒ“ê¸€ ìˆ˜
            comments = st.number_input("ëŒ“ê¸€ ìˆ˜ (ìœ íŠœë¸Œ/DC/FMKorea)", min_value=0, max_value=100, value=20, key="crawl_comments")
            
            # ê³ ê¸‰ ì˜µì…˜
            with st.expander("ê³ ê¸‰ ì˜µì…˜"):
                parallel = st.checkbox("ë³‘ë ¬ ì²˜ë¦¬", value=True, help="ì—¬ëŸ¬ í”Œë«í¼ì„ ë™ì‹œì— í¬ë¡¤ë§í•˜ì—¬ ì‹œê°„ì„ ì ˆì•½í•©ë‹ˆë‹¤.", key="crawl_parallel")
                no_sentiment = st.checkbox("ê°ì„± ë¶„ì„ ì œì™¸", value=False, help="ê°ì„± ë¶„ì„ì„ ì œì™¸í•˜ê³  í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.", key="crawl_no_sentiment")
                ignore_robots = st.checkbox("robots.txt ë¬´ì‹œ", value=False, help="ì›¹ì‚¬ì´íŠ¸ì˜ robots.txt ì •ì±…ì„ ë¬´ì‹œí•˜ê³  í¬ë¡¤ë§í•©ë‹ˆë‹¤. DCinsideì™€ FMKorea í¬ë¡¤ë§ì— í•„ìš”í•©ë‹ˆë‹¤.", key="crawl_ignore_robots")
                max_daily_queries = st.number_input("êµ¬ê¸€ API ì¼ì¼ ì¿¼ë¦¬ ì œí•œ", min_value=10, max_value=1000, value=100, help="êµ¬ê¸€ APIëŠ” ë¬´ë£Œ ê³„ì •ì—ì„œ ì¼ì¼ 100íšŒë¡œ ì œí•œë©ë‹ˆë‹¤.", key="crawl_google_limit")
            
            # í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼
            st.markdown("---")
            
            # í¬ë¡¤ë§ ìƒíƒœ í™•ì¸
            is_crawling = crawler_status['is_running']
            
            if not is_crawling:
                if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", use_container_width=True, type="primary", key="start_crawl_btn"):
                    if not region_keyword.strip():
                        st.error("ì§€ì—­ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ëª…ë ¹ì–´ êµ¬ì„±
                        cmd_parts = [
                            "python main.py",
                            f"--keywords {get_keywords_string(region_keyword, st.session_state.additional_keywords)}"
                        ]
                        
                        # í”Œë«í¼ ì˜µì…˜ ì²˜ë¦¬
                        if platforms:
                            platform_option = ",".join(platforms)
                            if platform_option.strip().lower() == "all":
                                st.error("í”Œë«í¼ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                                return
                            cmd_parts.append(f"--platform {platform_option}")
                        else:
                            st.error("í”Œë«í¼ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                            return
                        
                        # ë‚˜ë¨¸ì§€ ì˜µì…˜ ì¶”ê°€
                        cmd_parts.append(f"--max-pages {pages}")
                        cmd_parts.append(f"--max-comments {comments}")
                        
                        # ê³ ê¸‰ ì˜µì…˜ ì¶”ê°€
                        if parallel:
                            cmd_parts.append("--parallel")
                        if no_sentiment:
                            cmd_parts.append("--no-sentiment")
                        if ignore_robots:
                            cmd_parts.append("--respect-robots")
                        cmd_parts.append(f"--max-daily-queries {max_daily_queries}")
                        
                        # ìµœì¢… ëª…ë ¹ì–´
                        cmd = " ".join(cmd_parts)
                        
                        # ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ìƒì„±
                        os.makedirs("data/raw", exist_ok=True)
                        os.makedirs("data/logs", exist_ok=True)
                        
                        # ëª…ë ¹ì–´ ìƒíƒœ ì €ì¥
                        crawler_status['command'] = cmd
                        
                        # ë¡œê·¸ì— ëª…ë ¹ì–´ ê¸°ë¡
                        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {cmd}")
                        
                        try:
                            # í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘
                            crawling_thread = threading.Thread(
                                target=run_crawler,
                                args=(cmd,)
                            )
                            crawling_thread.daemon = True
                            crawling_thread.start()
                            logger.info(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ë¨: {crawling_thread.name}")
                        except Exception as e:
                            logger.error(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                            st.error(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                        
                        # í™”ë©´ ìƒˆë¡œê³ ì¹¨
                        time.sleep(0.5)  # ìŠ¤ë ˆë“œê°€ ì‹œì‘ë˜ê¸°ë¥¼ ì ì‹œ ê¸°ë‹¤ë¦¼
                        st.rerun()
            else:
                if st.button("â¹ï¸ í¬ë¡¤ë§ ì¤‘ì§€", use_container_width=True, type="secondary", key="stop_crawl_btn"):
                    # ì‹¤ì œë¡œ ìŠ¤ë ˆë“œë¥¼ ì¤‘ì§€í•  ìˆ˜ ì—†ì§€ë§Œ UIì—ì„œëŠ” í¬ë¡¤ë§ì´ ì¤‘ì§€ëœ ê²ƒì²˜ëŸ¼ í‘œì‹œ
                    crawler_status['is_running'] = False
                    crawler_status['message'] = "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
                    crawler_status['progress'] = 1.0
                    crawler_status['update_timestamp'] = time.time()
                    save_crawler_status(crawler_status)  # ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸
                    logger.info("í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­")
                    st.warning("í¬ë¡¤ë§ ìŠ¤ë ˆë“œë¥¼ ê°•ì œë¡œ ì¤‘ë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                    st.rerun()
        
        elif dashboard_mode == "ë°ì´í„° ë¶„ì„":
            st.header("ğŸ“Š ë°ì´í„° ë¶„ì„")
            
            # ë°ì´í„°ì…‹ ì„ íƒ
            datasets = get_available_datasets()
            if not datasets:
                st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ë°ì´í„°ì…‹ ì„ íƒ
            selected_dataset = st.selectbox(
                "ë¶„ì„í•  ë°ì´í„°ì…‹ ì„ íƒ",
                options=[d['filename'] for d in datasets],
                format_func=lambda x: f"{x} ({datasets[[d['filename'] for d in datasets].index(x)]['count']}ê°œ)"
            )
            
            # ê°ì„±ë¶„ì„ ëª¨ë¸ ì„ íƒ
            data_processor = DataProcessor()
            available_models = data_processor.get_available_models()
            model_combinations = data_processor.model_combinations
            
            st.subheader("ëª¨ë¸ ì„ íƒ")
            
            # ê°œë³„ ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ ì˜µì…˜)
            selected_models = st.multiselect(
                "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
                options=list(available_models.keys()),
                format_func=lambda x: available_models[x],
                default=['kobert', 'kcbert'],
                help="ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ì•™ìƒë¸”ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
            )
            
            # ë¯¸ë¦¬ ì •ì˜ëœ ì¡°í•© ì„ íƒ (ë³´ì¡° ì˜µì…˜)
            st.markdown("---")
            st.subheader("ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ë¸ ì¡°í•©")
            
            # ì¡°í•© ì„¤ëª…ì„ ë” ê°€ë…ì„± ìˆê²Œ í‘œì‹œ
            combinations_info = {
                'light': {
                    'title': 'ê°€ë²¼ìš´ ì¡°í•©',
                    'models': ['kobert', 'kcelectra'],
                    'description': 'ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì— ìµœì í™”ëœ ì¡°í•©ì…ë‹ˆë‹¤.'
                },
                'balanced': {
                    'title': 'ê· í˜•ì¡íŒ ì¡°í•©',
                    'models': ['kcbert', 'kcelectra', 'kosentencebert'],
                    'description': 'ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•ì„ ë§ì¶˜ ì¡°í•©ì…ë‹ˆë‹¤.'
                },
                'heavy': {
                    'title': 'ì •í™•ë„ ì¤‘ì‹¬ ì¡°í•©',
                    'models': ['kcbert-large', 'kosentencebert', 'kcelectra'],
                    'description': 'ë†’ì€ ì •í™•ë„ë¥¼ ìš°ì„ ì‹œí•˜ëŠ” ì¡°í•©ì…ë‹ˆë‹¤.'
                }
            }
            
            # ì¡°í•© ì„ íƒ UI
            for combo_key, combo_info in combinations_info.items():
                with st.expander(f"ğŸ“Š {combo_info['title']}"):
                    st.markdown(f"**í¬í•¨ ëª¨ë¸:** {', '.join(combo_info['models'])}")
                    st.markdown(f"*{combo_info['description']}*")
                    if st.button(f"ì´ ì¡°í•©ìœ¼ë¡œ ë¶„ì„í•˜ê¸°", key=f"use_{combo_key}"):
                        try:
                            with st.spinner("ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘..."):
                                # ì„ íƒëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                                filepath = next(d['path'] for d in datasets if d['filename'] == selected_dataset)
                                
                                # ì„ íƒëœ ì¡°í•©ìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰
                                df = data_processor.analyze_dataset(
                                    input_file=filepath,
                                    models=combo_info['models'],
                                    output_dir="data/processed"
                                )
                                
                                if df is not None and not df.empty:
                                    st.session_state.analysis_data = df
                                    st.session_state.show_results = True
                                    st.session_state.last_dataset = selected_dataset
                                    st.session_state.last_models = combo_info['models']
                                    st.rerun()
                                else:
                                    st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            st.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            
            # ê°œë³„ ëª¨ë¸ ì„ íƒ ì‹œ ë¶„ì„ ë²„íŠ¼
            if selected_models and st.button("ì„ íƒí•œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°"):
                try:
                    with st.spinner("ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘..."):
                        # ì„ íƒëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                        filepath = next(d['path'] for d in datasets if d['filename'] == selected_dataset)
                        
                        # ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤í–‰ (íŒŒì¼ ê²½ë¡œì™€ ëª¨ë¸ ëª©ë¡ ì „ë‹¬)
                        df = data_processor.analyze_dataset(
                            input_file=filepath,
                            models=selected_models,
                            output_dir="data/processed"
                        )
                        
                        if df is not None and not df.empty:
                            st.session_state.analysis_data = df
                            st.session_state.show_results = True
                            st.session_state.last_dataset = selected_dataset
                            st.session_state.last_models = selected_models
                            st.rerun()
                        else:
                            st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
    
    # ë©”ì¸ ì˜ì—­ - í¬ë¡¤ë§ ëª¨ë“œì¼ ë•Œ ìƒíƒœ í‘œì‹œ
    if dashboard_mode == "í¬ë¡¤ë§":
        st.header("ğŸ¤– í¬ë¡¤ë§ ìƒíƒœ ëª¨ë‹ˆí„°ë§")
        
        # í¬ë¡¤ë§ ìƒíƒœ í™•ì¸
        is_crawling = crawler_status['is_running']
        
        if is_crawling:
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            st.subheader("ì§„í–‰ ìƒí™©")
            progress_value = float(crawler_status['progress'])
            st.progress(progress_value, text=f"{int(progress_value * 100)}% ì™„ë£Œ")
            
            # í˜„ì¬ ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œ
            st.info(crawler_status['message'])
            
            # ëª…ë ¹ì–´ í‘œì‹œ
            with st.expander("ì‹¤í–‰ ì¤‘ì¸ ëª…ë ¹ì–´ í™•ì¸"):
                st.code(crawler_status['command'])
                
            # ìë™ ìƒˆë¡œê³ ì¹¨ (5ì´ˆë§ˆë‹¤)
            time_since_update = time.time() - crawler_status['update_timestamp']
            if time_since_update > 10:
                st.warning(f"ì—…ë°ì´íŠ¸ ì—†ìŒ: {int(time_since_update)}ì´ˆ ë™ì•ˆ ìƒíƒœ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë©”íƒ€ë°ì´í„°
            st.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {datetime.fromtimestamp(crawler_status['update_timestamp']).strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            # ìµœê·¼ ê²°ê³¼ í‘œì‹œ
            st.subheader("ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼")
            results = load_latest_results()
            
            if results:
                # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
                st.write("ìµœê·¼ ìˆ˜ì§‘ëœ ë°ì´í„°ì…‹:")
                result_df = pd.DataFrame([
                    {
                        "í”Œë«í¼": r["platform"],
                        "í‚¤ì›Œë“œ": r["keywords"],
                        "í•­ëª© ìˆ˜": r["items"],
                        "ìˆ˜ì§‘ ì‹œê°„": r["modified"],
                        "íŒŒì¼ëª…": r["filename"]
                    } for r in results
                ])
                st.dataframe(result_df, use_container_width=True)
                
                # í†µê³„ í‘œì‹œ
                stats = get_latest_result_stats()
                if stats:
                    st.subheader("í†µê³„ ìš”ì•½")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("ì´ ìˆ˜ì§‘ í•­ëª©", stats['total'])
                        
                        # í”Œë«í¼ë³„ í•­ëª© ìˆ˜
                        st.write("í”Œë«í¼ë³„ í•­ëª© ìˆ˜:")
                        platform_data = [{"í”Œë«í¼": p, "í•­ëª© ìˆ˜": c} for p, c in stats['platform_counts'].items()]
                        platform_df = pd.DataFrame(platform_data)
                        st.dataframe(platform_df, use_container_width=True)
                    
                    with col2:
                        # ê°ì„± ë¶„í¬
                        sentiment_data = [{"ê°ì„±": s, "í•­ëª© ìˆ˜": c} for s, c in stats['sentiment_counts'].items()]
                        sentiment_df = pd.DataFrame(sentiment_data)
                        
                        # ì°¨íŠ¸
                        fig, ax = plt.subplots()
                        bars = ax.bar(sentiment_df['ê°ì„±'], sentiment_df['í•­ëª© ìˆ˜'])
                        
                        # ìƒ‰ìƒ ì„¤ì •
                        colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'unknown': 'lightgray'}
                        for i, bar in enumerate(bars):
                            sentiment = sentiment_df.iloc[i]['ê°ì„±']
                            bar.set_color(colors.get(sentiment, 'blue'))
                            
                        plt.title("ê°ì„± ë¶„í¬")
                        st.pyplot(fig)
                    
                    # ìµœê·¼ í¬ë¡¤ë§ ì‹œê°„
                    st.caption(f"ìµœê·¼ í¬ë¡¤ë§: {results[0]['modified'] if results else 'ì—†ìŒ'}")
            else:
                st.info("ì•„ì§ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
            # ë°ì´í„° ìœ„ì¹˜ ì•ˆë‚´
            st.markdown("---")
            st.markdown("##### ìˆ˜ì§‘ëœ ë°ì´í„° ìœ„ì¹˜")
            st.code("data/raw/*.json")
            st.caption("ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ìœ„ ê²½ë¡œì— JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ëª¨ë“œì—ì„œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ë©”ì¸ ì˜ì—­ - ë°ì´í„° ë¶„ì„ ëª¨ë“œì¼ ë•Œ ê²°ê³¼ í‘œì‹œ
    if dashboard_mode == "ë°ì´í„° ë¶„ì„":
        # ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
        if st.session_state.analyzer_option == "Naive Bayes":
            sentiment_analyzer = SentimentAnalyzer()
        elif st.session_state.analyzer_option == "KoBERT":
            sentiment_analyzer = KoBERTSentimentAnalyzer()
        elif st.session_state.analyzer_option == "Ensemble":
            sentiment_analyzer = EnsembleSentimentAnalyzer()
    
        # ê¸°ë³¸ ì‹œê°í™” í‘œì‹œ (ë°ì´í„° ìœ ë¬´ì™€ ê´€ê³„ì—†ì´)
        st.subheader("ê°ì„± ë¶„í¬ ì§€ë„")
        if st.session_state.analysis_data is not None and isinstance(st.session_state.analysis_data, (pd.DataFrame, list)) and len(st.session_state.analysis_data) > 0:
            map_ = create_map(st.session_state.analysis_data)
            if map_ is not None:
                st_folium(map_, width=700, height=500)
            else:
                st.info("ì§€ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ê¸°ë³¸ ì§€ë„ ìƒì„± (ë¶€ì•ˆêµ° ì¤‘ì‹¬)
            map_ = folium.Map(location=[35.728, 126.733], zoom_start=10)
            marker_cluster = MarkerCluster().add_to(map_)
            folium.Marker(
                location=[35.728, 126.733],
                popup="ë¶€ì•ˆêµ°",
                icon=folium.Icon(color='blue')
            ).add_to(marker_cluster)
            st_folium(map_, width=700, height=500)
        
        # ê°ì„± ë¶„ì„ ìƒ˜í”Œ ì¶œë ¥
        st.subheader("ğŸ“‹ ê°ì„± ë¶„ì„ ìƒ˜í”Œ")
        if st.session_state.get("show_results", False) and st.session_state.get("analysis_data") is not None:
            df = st.session_state.analysis_data
            if not df.empty and all(col in df.columns for col in ['title', 'content', 'sentiment', 'confidence']):
                st.write(df[['title', 'content', 'sentiment', 'confidence']].head())
            else:
                st.warning("í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # CSV ë‹¤ìš´ë¡œë“œ
        if st.session_state.analysis_data is not None and isinstance(st.session_state.analysis_data, (pd.DataFrame, list)) and len(st.session_state.analysis_data) > 0:
            df = pd.DataFrame(st.session_state.analysis_data) if isinstance(st.session_state.analysis_data, list) else st.session_state.analysis_data
            if not df.empty:
                st.download_button(
                    "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                    df.to_csv(index=False).encode('utf-8'),
                    file_name=f"sentiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime='text/csv'
                )
            else:
                st.warning("ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°ì„± ë¶„í¬ ì‹œê°í™”
        st.subheader("ê°ì„± ë¶„í¬")
        if st.session_state.analysis_data is not None and isinstance(st.session_state.analysis_data, (pd.DataFrame, list)) and len(st.session_state.analysis_data) > 0:
            df = pd.DataFrame(st.session_state.analysis_data) if isinstance(st.session_state.analysis_data, list) else st.session_state.analysis_data
            if not df.empty and 'sentiment' in df.columns:
                col1, col2 = st.columns(2)
                with col1:
                    sentiment_counts = df['sentiment'].value_counts()
                    fig, ax = plt.subplots(figsize=(10, 6))
                    sentiment_counts.plot(kind='bar', ax=ax)
                    plt.title("ê°ì„± ë¶„í¬")
                    st.pyplot(fig)
                with col2:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    sentiment_counts.plot(kind='pie', autopct='%1.1f%%', ax=ax)
                    plt.title("ê°ì„± ë¶„í¬ (ë¹„ìœ¨)")
                    st.pyplot(fig)
            else:
                st.warning("ê°ì„± ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹œê³„ì—´ íŠ¸ë Œë“œ
        st.subheader("ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ")
        if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
            df = pd.DataFrame(st.session_state.analysis_data)
            
            # ë‚ ì§œ í˜•ì‹ ì •ê·œí™” ë° ë³€í™˜
            valid_dates = []
            for idx, row in df.iterrows():
                try:
                    date_str = row['published_date']
                    if date_str.isdigit() and len(date_str) == 8:
                        df.at[idx, 'date'] = pd.to_datetime(date_str, format='%Y%m%d')
                        valid_dates.append(idx)
                except:
                    pass
            
            # ìœ íš¨í•œ ë‚ ì§œë§Œ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df_valid = df.loc[valid_dates]
            
            if not df_valid.empty:
                daily_sentiment = df_valid.groupby(['date', 'sentiment']).size().unstack(fill_value=0)
                fig, ax = plt.subplots(figsize=(12, 6))
                daily_sentiment.plot(kind='line', ax=ax)
                plt.title("ì¼ë³„ ê°ì„± íŠ¸ë Œë“œ")
                st.pyplot(fig)
            else:
                st.warning("ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ í‘œì‹œí•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì›Œë“œí´ë¼ìš°ë“œ
        st.subheader("í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
        if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
            df = pd.DataFrame(st.session_state.analysis_data)
            col1, col2 = st.columns(2)
            with col1:
                # ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ
                text = ' '.join(df['content'])
                wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                plt.title("ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ")
                st.pyplot(fig)
            with col2:
                # ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ
                positive_text = ' '.join(df[df['sentiment'] == 'positive']['content'])
                if positive_text.strip():
                    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
                    fig, ax = plt.subplots(figsize=(10, 5))
                    ax.imshow(wordcloud, interpolation='bilinear')
                    ax.axis('off')
                    plt.title("ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ")
                    st.pyplot(fig)
                else:
                    st.write("ê¸ì • ê°ì„±ì˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # GPT ë¦¬í¬íŠ¸ ìƒì„±
        st.subheader("ì •ì±… ì œì•ˆ ë¦¬í¬íŠ¸")
        if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
            try:
                df = pd.DataFrame(st.session_state.analysis_data)
                report_generator = GPTReportGenerator(api_key=os.getenv("OPENAI_API_KEY"))
                report = report_generator.generate_report(df)
                st.text(report)
                
                # PDF ë¦¬í¬íŠ¸ ì €ì¥
                pdf_generator = PDFReportGenerator()
                pdf_path = pdf_generator.generate_pdf(report)
                if pdf_path:
                    with open(pdf_path, "rb") as f:
                        st.download_button(
                            "ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            f,
                            file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                            mime='application/pdf'
                        )
            except Exception as e:
                st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        else:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 