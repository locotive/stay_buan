import streamlit as st
import folium
from streamlit_folium import st_folium
from utils.data_loader import DataLoader
from core.sentiment_analysis import SentimentAnalyzer
from core.sentiment_analysis_kobert import KoBERTSentimentAnalyzer
from core.sentiment_analysis_ensemble import EnsembleSentimentAnalyzer
from reporting.report_generator_gpt import GPTReportGenerator
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from folium.plugins import MarkerCluster
from reporting.pdf_report_generator import PDFReportGenerator
from crawlers.naver_api_crawler import NaverSearchAPICrawler
from crawlers.youtube import YouTubeCrawler
import os
import glob
from collections import Counter
from dateutil import parser
import json
import random

@st.cache_data
def analyze_sentiment(text, analyzer):
    """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ê²°ê³¼ ìºì‹±"""
    sentiment, confidence = analyzer.predict(text)
    sentiment_label = 'negative' if sentiment == 0 else 'neutral' if sentiment == 1 else 'positive'
    return sentiment_label, confidence

def create_map(data):
    """Folium ì§€ë„ ìƒì„± ë° í´ëŸ¬ìŠ¤í„°ë§"""
    m = folium.Map(location=[35.728, 126.733], zoom_start=10)
    marker_cluster = MarkerCluster().add_to(m)
    
    for item in data:
        try:
            # ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë§ˆì»¤ ì¶”ê°€
            if 'lat' in item and 'lon' in item:
                folium.Marker(
                    location=[item['lat'], item['lon']],
                    popup=item['content'],
                    icon=folium.Icon(color='blue' if item['sentiment'] == 'positive' else 'red' if item['sentiment'] == 'negative' else 'gray')
                ).add_to(marker_cluster)
            # ìœ„ì¹˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ëœë¤ ì˜¤í”„ì…‹ ì‚¬ìš©
            else:
                lat = 35.728 + random.uniform(-0.01, 0.01)
                lon = 126.733 + random.uniform(-0.01, 0.01)
                folium.Marker(
                    location=[lat, lon],
                    popup=item['content'],
                    icon=folium.Icon(color='blue' if item['sentiment'] == 'positive' else 'red' if item['sentiment'] == 'negative' else 'gray')
                ).add_to(marker_cluster)
        except Exception as e:
            st.warning(f"ë§ˆì»¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue
    
    return m

def get_available_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜"""
    datasets = []
    for file in glob.glob("data/raw/naver_blog_*.json"):
        filename = os.path.basename(file)
        parts = filename.split('_')
        if len(parts) >= 4:
            count = parts[2]
            keywords = '_'.join(parts[3:-2])
            datasets.append({
                'filename': filename,
                'count': count,
                'keywords': keywords,
                'path': file
            })
    return sorted(datasets, key=lambda x: x['count'], reverse=True)

def load_selected_dataset(filepath):
    """ì„ íƒëœ ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def main():
    """ëŒ€ì‹œë³´ë“œ ë©”ì¸ í•¨ìˆ˜"""
    st.title("ë¶€ì•ˆêµ° ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'analyzer_option' not in st.session_state:
        st.session_state.analyzer_option = "Naive Bayes"
    if 'show_results' not in st.session_state:
        st.session_state.show_results = False
    if 'region_keyword' not in st.session_state:
        st.session_state.region_keyword = "ë¶€ì•ˆ"
    if 'additional_keywords' not in st.session_state:
        st.session_state.additional_keywords = [{"text": "", "condition": "AND"}]
    
    # ì‚¬ì´ë“œë°”ë¥¼ ìƒí•˜ë¡œ ë‚˜ëˆ„ê¸°
    with st.sidebar:
        # ìƒë‹¨: ë°ì´í„° ìˆ˜ì§‘ ì„¹ì…˜
        st.header("ë°ì´í„° ìˆ˜ì§‘")
        selected_platform = st.selectbox(
            "í”Œë«í¼ ì„ íƒ",
            ["naver_blog", "youtube", "twitter"],
            help="í˜„ì¬ëŠ” ë„¤ì´ë²„ ë¸”ë¡œê·¸ë§Œ ì§€ì›ë©ë‹ˆë‹¤. ë‹¤ë¥¸ í”Œë«í¼ì€ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤."
        )
        
        num_pages = st.number_input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=10000, value=3)
        
        # í‚¤ì›Œë“œ ì…ë ¥ UI
        st.subheader("í‚¤ì›Œë“œ ì„¤ì •")
        
        # ì§€ì—­ í‚¤ì›Œë“œ (í•„ìˆ˜)
        region_keyword = st.text_input("ì§€ì—­ í‚¤ì›Œë“œ", value=st.session_state.region_keyword)
        if region_keyword.strip():
            st.session_state.region_keyword = region_keyword
        else:
            st.error("ì§€ì—­ í‚¤ì›Œë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        
        # ì¶”ê°€ í‚¤ì›Œë“œ ì„¹ì…˜
        st.markdown("##### ì¶”ê°€ í‚¤ì›Œë“œ")
        
        # í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ ì¶”ê°€/ì œê±° ë²„íŠ¼
        col1, col2 = st.columns([1, 1])
        with col1:
            if st.button("â• í‚¤ì›Œë“œ ì¶”ê°€", use_container_width=True):
                st.session_state.additional_keywords.append({"text": "", "condition": "AND"})
        with col2:
            if st.button("â– í‚¤ì›Œë“œ ì œê±°", use_container_width=True) and len(st.session_state.additional_keywords) > 1:
                st.session_state.additional_keywords.pop()
        
        # ì¶”ê°€ í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œë“¤
        for i in range(len(st.session_state.additional_keywords)):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.session_state.additional_keywords[i]["text"] = st.text_input(
                    f"ì¶”ê°€ í‚¤ì›Œë“œ {i+1}",
                    value=st.session_state.additional_keywords[i]["text"],
                    key=f"keyword_{i}"
                )
            with col2:
                st.session_state.additional_keywords[i]["condition"] = st.selectbox(
                    "ì¡°ê±´",
                    ["AND", "OR"],
                    key=f"condition_{i}"
                )
        
        # ë°ì´í„° ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰", key="crawl_button"):
            if not st.session_state.region_keyword.strip():
                st.error("âŒ ì§€ì—­ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
                keywords = [{"text": st.session_state.region_keyword, "condition": "AND"}]
                keywords.extend([k for k in st.session_state.additional_keywords if k["text"].strip()])
                
                if selected_platform == "naver_blog":
                    crawler = NaverSearchAPICrawler(
                        keywords=keywords,
                        max_pages=num_pages
                    )
                    data = crawler.crawl()
                    if data:
                        st.session_state.analysis_data = data
                        st.session_state.show_results = True
                        st.success(f"âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ({len(data)}ê°œ)")
                    else:
                        st.error("âŒ ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                elif selected_platform == "youtube":
                    crawler = YouTubeCrawler(keywords=[st.session_state.region_keyword], max_results=30)
                    data = crawler.crawl()
                    if data:
                        st.session_state.analysis_data = data
                        st.session_state.show_results = True
                        st.success("âœ… ìœ íŠœë¸Œ í¬ë¡¤ë§ ì™„ë£Œ!")
                    else:
                        st.error("âŒ ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("âš ï¸ í•´ë‹¹ í”Œë«í¼ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.")
        
        # êµ¬ë¶„ì„  ì¶”ê°€
        st.markdown("---")
        
        # í•˜ë‹¨: ë¶„ì„ ì„¤ì • ì„¹ì…˜
        st.header("ë¶„ì„ ì„¤ì •")
        
        # ë°ì´í„°ì…‹ ì„ íƒ
        st.subheader("ë°ì´í„°ì…‹ ì„ íƒ")
        datasets = get_available_datasets()
        if datasets:
            dataset_options = {f"{d['count']}ê°œ - {d['keywords']}": d['path'] for d in datasets}
            selected_dataset = st.selectbox(
                "í¬ë¡¤ë§ëœ ë°ì´í„°ì…‹",
                options=list(dataset_options.keys())
            )
            selected_filepath = dataset_options[selected_dataset]
            data = load_selected_dataset(selected_filepath)
            
            # ë‚ ì§œ ë²”ìœ„ ì„ íƒ
            st.subheader("ë‚ ì§œ ë²”ìœ„ ì„ íƒ")
            dates = sorted(list(set(item['published_date'] for item in data)))
            if dates:
                min_date = datetime.strptime(dates[0], "%Y%m%d")
                max_date = datetime.strptime(dates[-1], "%Y%m%d")
                date_range = st.date_input(
                    "ë¶„ì„ ê¸°ê°„ ì„ íƒ",
                    value=(min_date, max_date),
                    min_value=min_date,
                    max_value=max_date
                )
                
                # ë‚ ì§œ í•„í„°ë§
                start_date = datetime.combine(date_range[0], datetime.min.time())
                end_date = datetime.combine(date_range[1], datetime.max.time())
                data = [
                    item for item in data 
                    if start_date <= datetime.strptime(item['published_date'], "%Y%m%d") <= end_date
                ]
        else:
            st.warning("ì €ì¥ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
            data = []
        
        # ê°ì„± ë¶„ì„ê¸° ì„ íƒ
        st.subheader("ê°ì„± ë¶„ì„ê¸° ì„ íƒ")
        analyzer_option = st.selectbox("ê°ì„± ë¶„ì„ê¸° ì„ íƒ", ["Naive Bayes", "KoBERT", "Ensemble"])
        
        # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
        if st.button("ğŸ” ë¶„ì„ ì‹¤í–‰", key="analyze_button"):
            if data:
                st.session_state.analysis_data = data
                st.session_state.analyzer_option = analyzer_option
                st.session_state.show_results = True
                st.success("âœ… ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # ë©”ì¸ í˜ì´ì§€ì— ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")
    
    # ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
    if st.session_state.analyzer_option == "Naive Bayes":
        sentiment_analyzer = SentimentAnalyzer()
    elif st.session_state.analyzer_option == "KoBERT":
        sentiment_analyzer = KoBERTSentimentAnalyzer()
    elif st.session_state.analyzer_option == "Ensemble":
        sentiment_analyzer = EnsembleSentimentAnalyzer()
    
    # ê¸°ë³¸ ì‹œê°í™” í‘œì‹œ (ë°ì´í„° ìœ ë¬´ì™€ ê´€ê³„ì—†ì´)
    st.subheader("ê°ì„± ë¶„í¬ ì§€ë„")
    if st.session_state.analysis_data:
        map_ = create_map(st.session_state.analysis_data)
    else:
        # ê¸°ë³¸ ì§€ë„ ìƒì„± (ë¶€ì•ˆêµ° ì¤‘ì‹¬)
        map_ = folium.Map(location=[35.728, 126.733], zoom_start=10)
        marker_cluster = MarkerCluster().add_to(map_)
        folium.Marker(
            location=[35.728, 126.733],
            popup="ë¶€ì•ˆêµ°",
            icon=folium.Icon(color='blue')
        ).add_to(marker_cluster)
    st_folium(map_, width=700, height=500)
    
    # ê°ì„± ë¶„ì„ ìƒ˜í”Œ ì¶œë ¥
    st.subheader("ğŸ“‹ ê°ì„± ë¶„ì„ ìƒ˜í”Œ")
    if st.session_state.analysis_data:
        df = pd.DataFrame(st.session_state.analysis_data)
        st.write(df[['title', 'content', 'sentiment', 'confidence']].head())
    else:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # CSV ë‹¤ìš´ë¡œë“œ
    if st.session_state.analysis_data:
        df = pd.DataFrame(st.session_state.analysis_data)
        st.download_button(
            "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
            df.to_csv(index=False).encode('utf-8'),
            file_name=f"sentiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime='text/csv'
        )
    
    # ê°ì„± ë¶„í¬ ì‹œê°í™”
    st.subheader("ê°ì„± ë¶„í¬")
    if st.session_state.analysis_data:
        df = pd.DataFrame(st.session_state.analysis_data)
        col1, col2 = st.columns(2)
        with col1:
            sentiment_counts = df['sentiment'].value_counts()
            fig, ax = plt.subplots(figsize=(10, 6))
            sentiment_counts.plot(kind='bar', ax=ax)
            plt.title("ê°ì„± ë¶„í¬")
            st.pyplot(fig)
        with col2:
            fig, ax = plt.subplots(figsize=(10, 6))
            sentiment_counts.plot(kind='pie', autopct='%1.1f%%', ax=ax)
            plt.title("ê°ì„± ë¶„í¬ (ë¹„ìœ¨)")
            st.pyplot(fig)
    else:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì‹œê³„ì—´ íŠ¸ë Œë“œ
    st.subheader("ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ")
    if st.session_state.analysis_data:
        df = pd.DataFrame(st.session_state.analysis_data)
        df['date'] = pd.to_datetime(df['published_date'])
        daily_sentiment = df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)
        fig, ax = plt.subplots(figsize=(12, 6))
        daily_sentiment.plot(kind='line', ax=ax)
        plt.title("ì¼ë³„ ê°ì„± íŠ¸ë Œë“œ")
        st.pyplot(fig)
    else:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì›Œë“œí´ë¼ìš°ë“œ
    st.subheader("í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    if st.session_state.analysis_data:
        df = pd.DataFrame(st.session_state.analysis_data)
        col1, col2 = st.columns(2)
        with col1:
            # ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ
            text = ' '.join(df['content'])
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            plt.title("ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ")
            st.pyplot(fig)
        with col2:
            # ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ
            positive_text = ' '.join(df[df['sentiment'] == 'positive']['content'])
            if positive_text.strip():
                wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                plt.title("ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ")
                st.pyplot(fig)
            else:
                st.write("ê¸ì • ê°ì„±ì˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # GPT ë¦¬í¬íŠ¸ ìƒì„±
    st.subheader("ì •ì±… ì œì•ˆ ë¦¬í¬íŠ¸")
    if st.session_state.analysis_data:
        try:
            df = pd.DataFrame(st.session_state.analysis_data)
            report_generator = GPTReportGenerator(api_key=os.getenv("OPENAI_API_KEY"))
            report = report_generator.generate_report(df)
            st.text(report)
            
            # PDF ë¦¬í¬íŠ¸ ì €ì¥
            pdf_generator = PDFReportGenerator()
            pdf_path = pdf_generator.generate_pdf(report)
            if pdf_path:
                with open(pdf_path, "rb") as f:
                    st.download_button(
                        "ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                        f,
                        file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                        mime='application/pdf'
                    )
        except Exception as e:
            st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 