import streamlit as st
import pandas as pd
import folium
from streamlit_folium import st_folium
from utils.data_loader import DataLoader
from utils.data_normalizer import load_and_normalize_data
from utils.data_processor import DataProcessor
from core.sentiment_analysis import SentimentAnalyzer
from core.sentiment_analysis_kobert import KoBERTSentimentAnalyzer
from core.sentiment_analysis_ensemble import EnsembleSentimentAnalyzer
from reporting.report_generator_gpt import GPTReportGenerator
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
from folium.plugins import MarkerCluster
from reporting.pdf_report_generator import PDFReportGenerator
from crawlers.naver_api_crawler import NaverSearchAPICrawler
from crawlers.youtube import YouTubeCrawler
from crawlers.google_search import GoogleSearchCrawler
import os
import glob
from collections import Counter
from dateutil import parser
import json
import random
import subprocess
import threading
import sys
from pathlib import Path
import re
import time
import queue
import logging
import concurrent.futures

# WordCloud í•œê¸€ í°íŠ¸ ê²½ë¡œ ì„¤ì •
def get_korean_font_path():
    possible_paths = [
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",  # macOS
        "/Library/Fonts/AppleGothic.ttf",              # macOS alternative
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Linux
        "C:/Windows/Fonts/malgun.ttf"                  # Windows
    ]
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

korean_font_path = get_korean_font_path()

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
def set_matplotlib_font():
    # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í°íŠ¸ ì¤‘ ë‚˜ëˆ”ê³ ë”• ë˜ëŠ” ë§‘ì€ ê³ ë”• ì°¾ê¸°
    font_list = [f.name for f in fm.fontManager.ttflist]
    nanum_fonts = [f for f in font_list if 'NanumGothic' in f]
    malgun_fonts = [f for f in font_list if 'Malgun Gothic' in f]
    
    # í°íŠ¸ ì„¤ì •
    if nanum_fonts:
        plt.rc('font', family=nanum_fonts[0])
        plt.rcParams['font.family'] = nanum_fonts[0]
    elif malgun_fonts:
        plt.rc('font', family=malgun_fonts[0])
        plt.rcParams['font.family'] = malgun_fonts[0]
    else:
        # í°íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        print("ê²½ê³ : í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ê¸°ë³¸ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'AppleGothic', 'DejaVu Sans']
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rc('axes', unicode_minus=False)
    plt.rcParams['axes.unicode_minus'] = False
    
    # í°íŠ¸ í¬ê¸° ì„¤ì •
    plt.rcParams['font.size'] = 12
    plt.rcParams['axes.titlesize'] = 14
    plt.rcParams['axes.labelsize'] = 12
    
    # í°íŠ¸ ì„¤ì • í™•ì¸
    print(f"í˜„ì¬ ì„¤ì •ëœ í°íŠ¸: {plt.rcParams['font.family']}")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ëª©ë¡: {[f.name for f in fm.fontManager.ttflist if 'Gothic' in f.name or 'Nanum' in f.name]}")

# í•œê¸€ í°íŠ¸ ì„¤ì • ì ìš©
set_matplotlib_font()
# ë¡œê¹… ì„¤ì •
os.makedirs("data/logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("data/logs/streamlit_dashboard.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("dashboard")

# í¬ë¡¤ë§ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
def save_crawler_status(status):
    """í¬ë¡¤ë§ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        os.makedirs("data/status", exist_ok=True)
        # ì‹œì‘ ì‹œê°„ì´ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
        if 'start_time' not in status and status['is_running']:
            status['start_time'] = time.time()
        with open("data/status/crawler_status.json", "w") as f:
            json.dump(status, f)
        logger.info("í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ë¨")
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)

def load_crawler_status():
    """íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ìƒíƒœ ì½ê¸°"""
    try:
        if os.path.exists("data/status/crawler_status.json"):
            with open("data/status/crawler_status.json", "r") as f:
                status = json.load(f)
            logger.info("í¬ë¡¤ë§ ìƒíƒœ ë¡œë“œë¨")
            return status
        else:
            return {
                'message': '',
                'progress': 0.0,
                'result': '',
                'is_running': False,
                'update_timestamp': 0,
                'command': ''
            }
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ìƒíƒœ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            'message': '',
            'progress': 0.0,
            'result': '',
            'is_running': False,
            'update_timestamp': 0,
            'command': ''
        }

# í¬ë¡¤ë§ ìƒíƒœë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
crawler_status = load_crawler_status()

# í¬ë¡¤ë§ ìŠ¤ë ˆë“œ í•¨ìˆ˜
def run_crawler(cmd):
    """í¬ë¡¤ë§ ëª…ë ¹ ì‹¤í–‰ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
    global crawler_status
    
    try:
        # ì´ˆê¸° ìƒíƒœ ì—…ë°ì´íŠ¸
        crawler_status['message'] = "í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘..."
        crawler_status['progress'] = 0.0
        crawler_status['result'] = ""
        crawler_status['is_running'] = True
        crawler_status['update_timestamp'] = time.time()
        crawler_status['start_time'] = time.time()
        crawler_status['command'] = cmd
        crawler_status['platform_progress'] = {}  # í”Œë«í¼ë³„ ì§„í–‰ë¥  ì €ì¥
        crawler_status['total_items'] = 0  # ì „ì²´ í•­ëª© ìˆ˜
        crawler_status['processed_items'] = 0  # ì²˜ë¦¬ëœ í•­ëª© ìˆ˜
        save_crawler_status(crawler_status)
        
        logger.info(f"í¬ë¡¤ë§ ëª…ë ¹ ì‹¤í–‰: {cmd}")
        
        with subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            bufsize=1,
            universal_newlines=True,
            shell=True
        ) as process:
            # í”Œë«í¼ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ - ëª…ë ¹ì–´ì—ì„œ í”Œë«í¼ ì •ë³´ ì¶”ì¶œ
            all_platforms = ["naver", "youtube", "google", "dcinside", "fmkorea", "buan"]
            
            # ëª…ë ¹ì–´ì—ì„œ platform íŒŒë¼ë¯¸í„° ì°¾ê¸°
            platform_param = ""
            for part in cmd.split():
                if part.startswith("--platform"):
                    platform_param = part.split("=")[1] if "=" in part else cmd.split()[cmd.split().index(part) + 1]
                    break
            
            # í”Œë«í¼ ëª©ë¡ ê²°ì •
            if platform_param == "all" or not platform_param:
                platforms = all_platforms
            else:
                platforms = platform_param.split(",")
                
            # í”Œë«í¼ë³„ ì§„í–‰ë¥  ì´ˆê¸°í™”
            for platform in platforms:
                crawler_status['platform_progress'][platform] = 0.0
            
            current_platform = None
            platform_index = 0
            lines = []
            
            logger.info("í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨, ì¶œë ¥ ëª¨ë‹ˆí„°ë§ ì¤‘...")
            
            for line in process.stdout:
                line = line.strip()
                lines.append(line)
                logger.info(f"í¬ë¡¤ë§ ì¶œë ¥: {line}")
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if "í¬ë¡¤ë§ ì‹œì‘" in line:
                    for platform in platforms:
                        if platform.upper() in line:
                            current_platform = platform
                            platform_index += 1
                            status_msg = f"í˜„ì¬ í”Œë«í¼: {current_platform.upper()} í¬ë¡¤ë§ ì¤‘... ({platform_index}/{len(platforms)})"
                            crawler_status['message'] = status_msg
                            crawler_status['update_timestamp'] = time.time()
                            save_crawler_status(crawler_status)
                            logger.info(status_msg)
                            break
                
                # í•­ëª© ìˆ˜ ì—…ë°ì´íŠ¸
                if "ìˆ˜ì§‘ëœ í•­ëª©:" in line:
                    try:
                        items_count = int(line.split("ìˆ˜ì§‘ëœ í•­ëª©:")[1].strip())
                        crawler_status['total_items'] = max(crawler_status['total_items'], items_count)
                        if current_platform:
                            crawler_status['platform_progress'][current_platform] = min(0.99, items_count / 100)  # ì„ì‹œ ì§„í–‰ë¥ 
                        save_crawler_status(crawler_status)
                    except:
                        pass
                
                # í”Œë«í¼ë³„ ì™„ë£Œ í™•ì¸
                if "í¬ë¡¤ë§ ì™„ë£Œ" in line:
                    for platform in platforms:
                        if platform.upper() in line:
                            crawler_status['platform_progress'][platform] = 1.0
                            crawler_status['processed_items'] += 1
                            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚°
                            total_progress = sum(crawler_status['platform_progress'].values()) / len(platforms)
                            crawler_status['progress'] = total_progress
                            crawler_status['update_timestamp'] = time.time()
                            save_crawler_status(crawler_status)
                            break
                
                # ìµœì¢… ê²°ê³¼ í™•ì¸
                if "í†µí•© ê²°ê³¼ ì €ì¥ ê²½ë¡œ" in line:
                    result_path = line.split("í†µí•© ê²°ê³¼ ì €ì¥ ê²½ë¡œ:")[1].strip()
                    crawler_status['result'] = f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_path}"
                    crawler_status['progress'] = 1.0
                    crawler_status['update_timestamp'] = time.time()
                    save_crawler_status(crawler_status)
                    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_path}")
            
            # í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            return_code = process.wait()
            logger.info(f"í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ, ë¦¬í„´ ì½”ë“œ: {return_code}")
            
            # í¬ë¡¤ë§ ì™„ë£Œ
            crawler_status['progress'] = 1.0
            crawler_status['message'] = "âœ… í¬ë¡¤ë§ ì™„ë£Œ!"
            crawler_status['is_running'] = False
            crawler_status['update_timestamp'] = time.time()
            save_crawler_status(crawler_status)
            logger.info("í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ")
            
            # ì „ì²´ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥
            log_filename = f"crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            os.makedirs("data/logs", exist_ok=True)
            with open(f"data/logs/{log_filename}", "w") as f:
                f.write("\n".join(lines))
            logger.info(f"í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_filename}")
    
    except Exception as e:
        error_msg = f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        crawler_status['message'] = error_msg
        crawler_status['progress'] = 1.0
        crawler_status['is_running'] = False
        crawler_status['update_timestamp'] = time.time()
        save_crawler_status(crawler_status)
        logger.error(error_msg, exc_info=True)
        
        # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
        error_log_filename = f"crawl_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        os.makedirs("data/logs", exist_ok=True)
        with open(f"data/logs/{error_log_filename}", "w") as f:
            f.write(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
            f.write(f"ëª…ë ¹ì–´: {cmd}\n")
            import traceback
            f.write(traceback.format_exc())
        logger.info(f"ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: {error_log_filename}")

def get_keywords_string(region_keyword, additional_keywords):
    """í‚¤ì›Œë“œ ëª©ë¡ì„ ëª…ë ¹ì¤„ ì¸ì í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    keywords = [region_keyword]
    for kw in additional_keywords:
        if kw["text"].strip():
            keywords.append(kw["text"].strip())
    return ' '.join([f'"{k}"' if ' ' in k else k for k in keywords])

# í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
def load_latest_results(num_files=5):
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
    result_files = []
    
    # í”Œë«í¼ë³„ ìµœê·¼ íŒŒì¼ ê²€ìƒ‰
    platforms = ["naver", "youtube", "google", "dcinside", "fmkorea", "buan", "combined"]
    
    for platform in platforms:
        files = glob.glob(f"data/raw/{platform}*.json")
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬
        files.sort(key=os.path.getmtime, reverse=True)
        # ìµœê·¼ íŒŒì¼ ì„ íƒ
        for file in files[:num_files]:
            if os.path.exists(file):
                try:
                    stats = os.stat(file)
                    mtime = datetime.fromtimestamp(stats.st_mtime)
                    
                    # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    filename = os.path.basename(file)
                    match = re.search(r'(\w+)_(\d+)_([^_]+)', filename)
                    
                    platform_name = ""
                    item_count = 0
                    keywords = ""
                    
                    if match:
                        platform_name = match.group(1)
                        item_count = int(match.group(2))
                        keywords = match.group(3)
                    else:
                        # ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ì—†ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ëŒ€ì²´
                        platform_name = platform
                        item_count = 0
                        keywords = filename
                    
                    result_files.append({
                        "filename": filename,
                        "platform": platform_name,
                        "keywords": keywords,
                        "items": item_count,
                        "path": file,
                        "modified": mtime.strftime("%Y-%m-%d %H:%M:%S")
                    })
                except Exception as e:
                    st.error(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
    
    # ì‹œê°„ìˆœ ì •ë ¬
    result_files.sort(key=lambda x: x["modified"], reverse=True)
    return result_files[:num_files]

def get_latest_result_stats():
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ í†µê³„"""
    try:
        # ëª¨ë“  í”Œë«í¼ì˜ ìµœê·¼ ê²°ê³¼ íŒŒì¼ ê²€ìƒ‰
        platforms = {
            "naver_news": "ë„¤ì´ë²„ ë‰´ìŠ¤",
            "naver_blog": "ë„¤ì´ë²„ ë¸”ë¡œê·¸",
            "naver_cafearticle": "ë„¤ì´ë²„ ì¹´í˜",
            "youtube": "ìœ íŠœë¸Œ",
            "google": "êµ¬ê¸€",
            "dcinside": "ë””ì‹œì¸ì‚¬ì´ë“œ",
            "fmkorea": "ì—í¨ì½”ë¦¬ì•„",
            "buan": "ë¶€ì•ˆêµ°ì²­"
        }
        
        all_results = []
        platform_counts = {}
        latest_files = {}
        
        # ê° í”Œë«í¼ë³„ ìµœê·¼ íŒŒì¼ ê²€ìƒ‰
        for platform in platforms.keys():
            files = glob.glob(f"data/raw/{platform}*.json")
            if files:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
                latest_file = max(files, key=os.path.getmtime)
                latest_files[platform] = latest_file
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            all_results.extend(data)
                            platform_counts[platforms[platform]] = len(data)
                            logger.info(f"{platforms[platform]}: {len(data)}ê°œ í•­ëª© ë¡œë“œë¨")
                except Exception as e:
                    logger.error(f"{platform} ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        if not all_results:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê°ì„±ë³„ í•­ëª© ìˆ˜
        sentiment_counts = {'positive': 0, 'neutral': 0, 'negative': 0, 'unknown': 0}
        for item in all_results:
            sentiment = item.get('sentiment')
            if sentiment is None:
                sentiment_counts['unknown'] += 1
            elif sentiment == 0 or sentiment == 'negative':
                sentiment_counts['negative'] += 1
            elif sentiment == 1 or sentiment == 'neutral':
                sentiment_counts['neutral'] += 1
            elif sentiment == 2 or sentiment == 'positive':
                sentiment_counts['positive'] += 1
            else:
                sentiment_counts['unknown'] += 1
        
        # ë‚ ì§œë³„ í•­ëª© ìˆ˜
        date_counts = {}
        for item in all_results:
            date = item.get('published_date', '')
            if date and len(date) >= 8:  # 'YYYYMMDD' í˜•ì‹
                try:
                    date = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
                except:
                    date = 'unknown'
            else:
                date = 'unknown'
            date_counts[date] = date_counts.get(date, 0) + 1
        
        # ê²°ê³¼ ë¡œê¹…
        logger.info(f"ì´ {len(all_results)}ê°œ í•­ëª© ì²˜ë¦¬ë¨")
        logger.info(f"í”Œë«í¼ë³„ í•­ëª© ìˆ˜: {platform_counts}")
        logger.info(f"ê°ì„±ë³„ í•­ëª© ìˆ˜: {sentiment_counts}")
        
        return {
            'total': len(all_results),
            'platform_counts': platform_counts,
            'sentiment_counts': sentiment_counts,
            'date_counts': date_counts,
            'latest_files': latest_files
        }
    except Exception as e:
        logger.error(f"ê²°ê³¼ í†µê³„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return None

@st.cache_data
def analyze_sentiment(text, analyzer):
    """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ê²°ê³¼ ìºì‹±"""
    sentiment, confidence = analyzer.predict(text)
    sentiment_label = 'negative' if sentiment == 0 else 'neutral' if sentiment == 1 else 'positive'
    return sentiment_label, confidence

def clear_analysis_cache():
    """ë¶„ì„ ê´€ë ¨ ìºì‹œì™€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    try:
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        for key in list(st.session_state.keys()):
            if key.startswith('analysis_') or key in ['show_results', 'last_dataset', 'last_models']:
                del st.session_state[key]
        
        # Streamlit ìºì‹œ ì´ˆê¸°í™”
        analyze_sentiment.clear()
        
        logger.info("ëª¨ë“  ë¶„ì„ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        logger.error(f"ìºì‹œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def get_available_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜"""
    # ë¶„ì„ ê²°ê³¼ íŒŒì¼ ëª©ë¡
    analysis_results = []
    processed_files = glob.glob("data/processed/sentiment_analysis_*.json")
    
    for json_file in processed_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # CSV íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            csv_file = metadata['output_files']['csv']
            if not os.path.exists(csv_file):
                continue
            
            # ë¶„ì„ ê²°ê³¼ ì •ë³´ êµ¬ì„±
            result_info = {
                'filename': os.path.basename(csv_file),
                'analysis_time': metadata['timestamp'],
                'models': metadata['models'],
                'item_count': metadata['item_count'],
                'sentiment_distribution': metadata['sentiment_distribution'],
                'csv_file': csv_file,
                'json_file': json_file,
                'input_file': metadata['input_file']
            }
            analysis_results.append(result_info)
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ë¶„ì„ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    analysis_results.sort(key=lambda x: x['analysis_time'], reverse=True)
    
    # ì›ë³¸ ë°ì´í„°ì…‹ ëª©ë¡
    raw_datasets = []
    for platform in ['naver', 'youtube', 'google', 'dcinside', 'fmkorea', 'buan']:
        for file in glob.glob(f"data/raw/{platform}_*.json"):
            try:
                filename = os.path.basename(file)
                parts = filename.split('_')
                
                if len(parts) >= 3:
                    if platform == 'youtube':
                        count = parts[1]
                        keywords = '_'.join(parts[2:-2])
                    else:
                        count = parts[1]
                        keywords = '_'.join(parts[2:-2])
                        
                    # ì´ë¯¸ ë¶„ì„ëœ íŒŒì¼ì¸ì§€ í™•ì¸
                    is_analyzed = any(r['input_file'] == file for r in analysis_results)
                    
                    raw_datasets.append({
                        'filename': filename,
                        'platform': platform,
                        'path': file,
                        'count': count,
                        'keywords': keywords,
                        'modified_time': datetime.fromtimestamp(os.path.getmtime(file)).strftime("%Y-%m-%d %H:%M:%S"),
                        'is_analyzed': is_analyzed
                    })
            except:
                continue
    
    # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    raw_datasets.sort(key=lambda x: x['modified_time'], reverse=True)
    
    return {
        'analysis_results': analysis_results,
        'raw_datasets': raw_datasets
    }

def load_selected_dataset(filepath):
    """ì„ íƒëœ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì •ê·œí™”"""
    try:
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        data_loader = DataLoader()
        data_processor = DataProcessor()
        
        # íŒŒì¼ëª…ì—ì„œ í”Œë«í¼ ì¶”ì¶œ
        platform = os.path.basename(filepath).split('_')[0]
        
        # ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
        df = load_and_normalize_data(filepath)
        
        # ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        df = data_processor.process_data(df, platform)
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        processed_filepath = data_processor.save_processed_data(df, platform)
        
        st.success(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {processed_filepath}")
        return df
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def estimate_processing_time(data_size, models_count):
    """ë°ì´í„° í¬ê¸°ì™€ ëª¨ë¸ ìˆ˜ì— ë”°ë¥¸ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°"""
    # ê° ëª¨ë¸ë³„ í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    model_processing_times = {
        'kobert': 0.5,
        'kcelectra-base-v2022': 0.4,
        'kcelectra': 0.4,
        'kcbert-large': 1.2,
        'kosentencebert': 0.8
    }
    
    # ê¸°ë³¸ ì²˜ë¦¬ ì‹œê°„ (ë°ì´í„° ë¡œë”©, ì „ì²˜ë¦¬ ë“±)
    base_time = 2.0
    
    # ëª¨ë¸ë³„ ì²˜ë¦¬ ì‹œê°„ í•©ì‚°
    total_model_time = sum(model_processing_times.get(model, 0.5) for model in models_count)
    
    # ì „ì²´ ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ì´ˆ)
    estimated_time = (base_time + total_model_time) * data_size
    
    return estimated_time

def format_time(seconds):
    """ì´ˆë¥¼ ì½ê¸° ì‰¬ìš´ ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if seconds < 60:
        return f"{int(seconds)}ì´ˆ"
    elif seconds < 3600:
        minutes = int(seconds / 60)
        remaining_seconds = int(seconds % 60)
        return f"{minutes}ë¶„ {remaining_seconds}ì´ˆ"
    else:
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours}ì‹œê°„ {minutes}ë¶„"

def update_analysis_progress(progress_bar, status_text, current, total, start_time):
    """ë¶„ì„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
    progress = current / total
    progress_bar.progress(progress)
    
    # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
    elapsed_time = time.time() - start_time
    # ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡
    if current > 0:
        estimated_total = (elapsed_time / current) * total
        remaining_time = estimated_total - elapsed_time
        status_text.text(f"ì§„í–‰ë¥ : {int(progress * 100)}% ({current}/{total} í•­ëª©)\n"
                        f"ê²½ê³¼ ì‹œê°„: {format_time(elapsed_time)}\n"
                        f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {format_time(remaining_time)}")
    else:
        status_text.text(f"ì§„í–‰ë¥ : 0% (0/{total} í•­ëª©)\n"
                        f"ê²½ê³¼ ì‹œê°„: {format_time(elapsed_time)}")

def main():
    """ëŒ€ì‹œë³´ë“œ ë©”ì¸ í•¨ìˆ˜"""
    global crawler_status
    
    # ì‹œì‘ ì‹œ í¬ë¡¤ë§ ìƒíƒœ íŒŒì¼ì—ì„œ ë¡œë“œ
    crawler_status = load_crawler_status()
    
    st.title("ë¶€ì•ˆêµ° ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'analyzer_option' not in st.session_state:
        st.session_state.analyzer_option = "Naive Bayes"
    if 'show_results' not in st.session_state:
        st.session_state.show_results = False
    if 'region_keyword' not in st.session_state:
        st.session_state.region_keyword = "ë¶€ì•ˆ"
    if 'additional_keywords' not in st.session_state:
        st.session_state.additional_keywords = [{"text": "", "condition": "AND"}]
    if 'dashboard_mode' not in st.session_state:
        st.session_state.dashboard_mode = "í¬ë¡¤ë§"
    if 'last_update_time' not in st.session_state:
        st.session_state.last_update_time = 0
    if 'analysis_start_time' not in st.session_state:
        st.session_state.analysis_start_time = None
    if 'analysis_progress' not in st.session_state:
        st.session_state.analysis_progress = 0
    if 'analysis_total' not in st.session_state:
        st.session_state.analysis_total = 0
    
    # ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€ì‹œë³´ë“œ ëª¨ë“œ ì„ íƒ
    with st.sidebar:
        st.header("ëŒ€ì‹œë³´ë“œ ëª¨ë“œ")
        dashboard_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["í¬ë¡¤ë§", "ë°ì´í„° ë¶„ì„"],
            index=0 if st.session_state.dashboard_mode == "í¬ë¡¤ë§" else 1,
            help="í¬ë¡¤ë§ ëª¨ë“œ: ë‹¤ì–‘í•œ í”Œë«í¼ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ëª¨ë“œ: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
        )
        st.session_state.dashboard_mode = dashboard_mode
        
        st.markdown("---")
        
        # ëª¨ë“œì— ë”°ë¼ ì‚¬ì´ë“œë°” ë‚´ìš© ë³€ê²½
        if dashboard_mode == "í¬ë¡¤ë§":
            st.header("í¬ë¡¤ë§ ì„¤ì •")
            
            # ì§€ì—­ í‚¤ì›Œë“œ (í•„ìˆ˜)
            region_keyword = st.text_input("ì§€ì—­ í‚¤ì›Œë“œ (í•„ìˆ˜)", value=st.session_state.region_keyword, key="crawl_region_keyword")
            if region_keyword.strip():
                st.session_state.region_keyword = region_keyword
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì„¹ì…˜
            st.markdown("##### ì¶”ê°€ í‚¤ì›Œë“œ")
            
            # í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ ì¶”ê°€/ì œê±° ë²„íŠ¼
            col_a, col_b = st.columns([1, 1])
            with col_a:
                if st.button("â• í‚¤ì›Œë“œ ì¶”ê°€", use_container_width=True, key="add_keyword_btn"):
                    st.session_state.additional_keywords.append({"text": "", "condition": "AND"})
                    st.rerun()
            with col_b:
                if st.button("â– í‚¤ì›Œë“œ ì œê±°", use_container_width=True, key="remove_keyword_btn") and len(st.session_state.additional_keywords) > 0:
                    st.session_state.additional_keywords.pop()
                    st.rerun()
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œë“¤
            for i in range(len(st.session_state.additional_keywords)):
                col_a, col_b = st.columns([3, 1])
                with col_a:
                    st.session_state.additional_keywords[i]["text"] = st.text_input(
                        f"í‚¤ì›Œë“œ {i+1}",
                        value=st.session_state.additional_keywords[i]["text"],
                        key=f"crawl_keyword_{i}"
                    )
                with col_b:
                    st.session_state.additional_keywords[i]["condition"] = st.selectbox(
                        "ì¡°ê±´",
                        ["AND", "OR"],
                        index=0 if st.session_state.additional_keywords[i]["condition"] == "AND" else 1,
                        key=f"crawl_condition_{i}"
                    )
            
            # í¬ë¡¤ë§ ì˜µì…˜ ì„¹ì…˜
            st.markdown("##### í¬ë¡¤ë§ ì˜µì…˜")
            
            # í”Œë«í¼ ì„ íƒ
            platform_groups = {
                "ë„¤ì´ë²„": ["naver"],
                "ìœ íŠœë¸Œ": ["youtube"],
                "êµ¬ê¸€": ["google"],
                "ì»¤ë®¤ë‹ˆí‹°": ["dcinside", "fmkorea"],
                "ë¶€ì•ˆêµ°ì²­": ["buan"]
            }
            
            selected_groups = st.multiselect(
                "í¬ë¡¤ë§í•  í”Œë«í¼ ì„ íƒ",
                options=list(platform_groups.keys()),
                default=list(platform_groups.keys()),
                help="ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥. ëª¨ë“  í”Œë«í¼ì„ ë™ì‹œì— í¬ë¡¤ë§í•˜ê±°ë‚˜ ì›í•˜ëŠ” í”Œë«í¼ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            # ì„ íƒëœ ê·¸ë£¹ì—ì„œ í”Œë«í¼ ëª©ë¡ ì¶”ì¶œ
            platforms = []
            for group in selected_groups:
                platforms.extend(platform_groups[group])
            
            platform_option = ",".join(platforms) if platforms else "all"
            
            # í˜ì´ì§€ ìˆ˜
            pages = st.number_input("í˜ì´ì§€/ê²°ê³¼ ìˆ˜", min_value=1, max_value=100, value=3, key="crawl_pages")
            
            # ëŒ“ê¸€ ìˆ˜
            comments = st.number_input("ëŒ“ê¸€ ìˆ˜ (ìœ íŠœë¸Œ/DC/FMKorea)", min_value=0, max_value=100, value=20, key="crawl_comments")
            
            # ê³ ê¸‰ ì˜µì…˜
            with st.expander("ê³ ê¸‰ ì˜µì…˜", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    browser_type = st.selectbox(
                        "ë¸Œë¼ìš°ì € ì„ íƒ",
                        ["chrome", "firefox"],
                        index=0,
                        help="í¬ë¡¤ë§ì— ì‚¬ìš©í•  ë¸Œë¼ìš°ì €ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."
                    )
                    max_daily_queries = st.number_input(
                        "êµ¬ê¸€ API ì¼ì¼ ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜",
                        min_value=10,
                        max_value=1000,
                        value=100,
                        step=10,
                        help="êµ¬ê¸€ APIì˜ ì¼ì¼ ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
                    )
                with col2:
                    respect_robots = st.checkbox(
                        "robots.txt ì •ì±… ì¤€ìˆ˜",
                        value=False,
                        help="ì›¹ì‚¬ì´íŠ¸ì˜ robots.txt ì •ì±…ì„ ì¤€ìˆ˜í•˜ë©° í¬ë¡¤ë§í•©ë‹ˆë‹¤."
                    )
                    perform_sentiment = st.checkbox(
                        "ê°ì„± ë¶„ì„ ìˆ˜í–‰",
                        value=False,
                        help="í¬ë¡¤ë§ ì¤‘ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                    )
            
            # í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼
            st.markdown("---")
            
            # í¬ë¡¤ë§ ìƒíƒœ í™•ì¸
            is_crawling = crawler_status['is_running']
            
            if not is_crawling:
                if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", use_container_width=True, type="primary", key="start_crawl_btn"):
                    if not region_keyword.strip():
                        st.error("ì§€ì—­ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ëª…ë ¹ì–´ êµ¬ì„±
                        cmd_parts = [
                            "python main.py",
                            f"--keywords {get_keywords_string(region_keyword, st.session_state.additional_keywords)}"
                        ]
                        
                        # í”Œë«í¼ ì˜µì…˜ ì²˜ë¦¬
                        if platforms:
                            platform_option = ",".join(platforms)
                            if platform_option.strip().lower() == "all":
                                st.error("í”Œë«í¼ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                                return
                            cmd_parts.append(f"--platform {platform_option}")
                        else:
                            st.error("í”Œë«í¼ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                            return
                        
                        # ë‚˜ë¨¸ì§€ ì˜µì…˜ ì¶”ê°€
                        cmd_parts.append(f"--max-pages {pages}")
                        cmd_parts.append(f"--max-comments {comments}")
                        
                        # ê³ ê¸‰ ì˜µì…˜ ì¶”ê°€
                        if browser_type != "chrome":
                            cmd_parts.append(f"--browser {browser_type}")
                        if respect_robots:
                            cmd_parts.append("--respect-robots")
                        if not perform_sentiment:
                            cmd_parts.append("--no-sentiment")
                        cmd_parts.append(f"--max-daily-queries {max_daily_queries}")
                        
                        # ìµœì¢… ëª…ë ¹ì–´
                        cmd = " ".join(cmd_parts)
                        
                        # ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ìƒì„±
                        os.makedirs("data/raw", exist_ok=True)
                        os.makedirs("data/logs", exist_ok=True)
                        
                        # ëª…ë ¹ì–´ ìƒíƒœ ì €ì¥
                        crawler_status['command'] = cmd
                        
                        # ë¡œê·¸ì— ëª…ë ¹ì–´ ê¸°ë¡
                        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {cmd}")
                        
                        try:
                            # í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘
                            crawling_thread = threading.Thread(
                                target=run_crawler,
                                args=(cmd,)
                            )
                            crawling_thread.daemon = True
                            crawling_thread.start()
                            logger.info(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ë¨: {crawling_thread.name}")
                        except Exception as e:
                            logger.error(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                            st.error(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                        
                        # í™”ë©´ ìƒˆë¡œê³ ì¹¨
                        time.sleep(0.5)  # ìŠ¤ë ˆë“œê°€ ì‹œì‘ë˜ê¸°ë¥¼ ì ì‹œ ê¸°ë‹¤ë¦¼
                        st.rerun()
            else:
                if st.button("â¹ï¸ í¬ë¡¤ë§ ì¤‘ì§€", use_container_width=True, type="secondary", key="stop_crawl_btn"):
                    # ì‹¤ì œë¡œ ìŠ¤ë ˆë“œë¥¼ ì¤‘ì§€í•  ìˆ˜ ì—†ì§€ë§Œ UIì—ì„œëŠ” í¬ë¡¤ë§ì´ ì¤‘ì§€ëœ ê²ƒì²˜ëŸ¼ í‘œì‹œ
                    crawler_status['is_running'] = False
                    crawler_status['message'] = "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
                    crawler_status['progress'] = 1.0
                    crawler_status['update_timestamp'] = time.time()
                    save_crawler_status(crawler_status)  # ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸
                    logger.info("í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­")
                    st.warning("í¬ë¡¤ë§ ìŠ¤ë ˆë“œë¥¼ ê°•ì œë¡œ ì¤‘ë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                    st.rerun()
        
        elif dashboard_mode == "ë°ì´í„° ë¶„ì„":
            st.header("ğŸ“Š ë°ì´í„° ë¶„ì„")
            
            # ì‚¬ì´ë“œë°”ì— ì›ë³¸ ë°ì´í„° ì„ íƒ UI ë°°ì¹˜
            with st.sidebar:
                st.header("ì›ë³¸ ë°ì´í„° ì„ íƒ")
                
                # ì›ë³¸ ë°ì´í„°ì…‹ ëª©ë¡
                datasets = get_available_datasets()
                raw_datasets = datasets['raw_datasets']
                
                if not raw_datasets:
                    st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ì›ë³¸ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # ì›ë³¸ ë°ì´í„°ì…‹ ì„ íƒ ì˜µì…˜ êµ¬ì„±
                    dataset_options = []
                    for dataset in raw_datasets:
                        if dataset['is_analyzed']:
                            label = f"ğŸ“Š {dataset['filename']} ({dataset['count']}ê°œ) - ì´ë¯¸ ë¶„ì„ë¨"
                        else:
                            label = f"ğŸ“„ {dataset['filename']} ({dataset['count']}ê°œ) - ë¶„ì„ ì „"
                        dataset_options.append((label, dataset))
                    
                    # ì›ë³¸ ë°ì´í„°ì…‹ ì„ íƒ
                    selected_label = st.selectbox(
                        "ë¶„ì„í•  ë°ì´í„°ì…‹ ì„ íƒ",
                        options=[opt[0] for opt in dataset_options],
                        help="ğŸ“ŠëŠ” ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì…‹, ğŸ“„ëŠ” ë¶„ì„ ì „ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤."
                    )
                    
                    # ì„ íƒëœ ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    selected_dataset = next(opt[1] for opt in dataset_options if opt[0] == selected_label)
                    
                    # ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì…‹ì¸ ê²½ìš° ê²½ê³ 
                    if selected_dataset['is_analyzed']:
                        st.warning("ì´ ë°ì´í„°ì…‹ì€ ì´ë¯¸ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    
            # ê°ì„±ë¶„ì„ ëª¨ë¸ ì„ íƒ
            data_processor = DataProcessor()
            available_models = data_processor.get_available_models()
            model_combinations = data_processor.model_combinations
            
            st.subheader("ëª¨ë¸ ì„ íƒ")
            
            # ê°œë³„ ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ ì˜µì…˜)
            selected_models = st.multiselect(
                "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
                options=list(available_models.keys()),
                format_func=lambda x: available_models[x],
                default=['kobert', 'kcelectra-base-v2022'],
                help="ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ì•™ìƒë¸”ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
            )
            
            # ë¯¸ë¦¬ ì •ì˜ëœ ì¡°í•© ì„ íƒ (ë³´ì¡° ì˜µì…˜)
            st.markdown("---")
            st.subheader("ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ë¸ ì¡°í•©")
            
            # ì¡°í•© ì„¤ëª…ì„ ë” ê°€ë…ì„± ìˆê²Œ í‘œì‹œ
            combinations_info = {
                'light': {
                    'title': 'ê°€ë²¼ìš´ ì¡°í•©',
                    'models': ['kobert', 'kcelectra-base-v2022'],
                    'description': 'ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì— ìµœì í™”ëœ ì¡°í•©ì…ë‹ˆë‹¤.'
                },
                'balanced': {
                    'title': 'ê· í˜•ì¡íŒ ì¡°í•©',
                    'models': ['kcelectra-base-v2022', 'kcelectra', 'kosentencebert'],
                    'description': 'ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•ì„ ë§ì¶˜ ì¡°í•©ì…ë‹ˆë‹¤.'
                },
                'heavy': {
                    'title': 'ì •í™•ë„ ì¤‘ì‹¬ ì¡°í•©',
                    'models': ['kcbert-large', 'kosentencebert', 'kcelectra-base-v2022'],
                    'description': 'ë†’ì€ ì •í™•ë„ë¥¼ ìš°ì„ ì‹œí•˜ëŠ” ì¡°í•©ì…ë‹ˆë‹¤.'
                }
            }
            
            # ì¡°í•© ì„ íƒ UI
            for combo_key, combo_info in combinations_info.items():
                with st.expander(f"ğŸ“Š {combo_info['title']}"):
                    st.markdown(f"**í¬í•¨ ëª¨ë¸:** {', '.join(combo_info['models'])}")
                    st.markdown(f"*{combo_info['description']}*")
                    if st.button(f"ì´ ì¡°í•©ìœ¼ë¡œ ë¶„ì„í•˜ê¸°", key=f"use_{combo_key}"):
                        try:
                            with st.spinner("ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘..."):
                                # ì„ íƒëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                                filepath = selected_dataset['path']
                                
                                # ì„ íƒëœ ì¡°í•©ìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰
                                df = data_processor.analyze_dataset(
                                    input_file=filepath,
                                    models=combo_info['models'],
                                    output_dir="data/processed"
                                )
                                
                                if df is not None and not df.empty:
                                    st.session_state.analysis_data = df
                                    st.session_state.show_results = True
                                    st.session_state.last_dataset = selected_dataset['filename']
                                    st.session_state.last_models = combo_info['models']
                                    st.rerun()
                                else:
                                    st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            st.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            
            # ê°œë³„ ëª¨ë¸ ì„ íƒ ì‹œ ë¶„ì„ ë²„íŠ¼
            if selected_models and st.button("ì„ íƒí•œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°"):
                try:
                    # ì„ íƒëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                    filepath = selected_dataset['path']
                    
                    # ë°ì´í„° í¬ê¸° í™•ì¸
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        data_size = len(data)
                    
                    # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
                    estimated_time = estimate_processing_time(data_size, selected_models)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
                    progress_container = st.empty()
                    status_container = st.empty()
                    progress_bar = progress_container.progress(0)
                    status_text = status_container.text("ë¶„ì„ ì¤€ë¹„ ì¤‘...")
                    
                    # ë¶„ì„ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                    st.session_state.analysis_start_time = time.time()
                    st.session_state.analysis_progress = 0
                    st.session_state.analysis_total = data_size
                    
                    # ì˜ˆìƒ ì‹œê°„ í‘œì‹œ
                    st.info(f"ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {format_time(estimated_time)} (ë°ì´í„° {data_size}ê°œ, ëª¨ë¸ {len(selected_models)}ê°œ)")
                    
                    with st.spinner("ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘..."):
                        # ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤í–‰
                        df = data_processor.analyze_dataset(
                            input_file=filepath,
                            models=selected_models,
                            output_dir="data/processed",
                            progress_callback=lambda current: update_analysis_progress(
                                progress_bar,
                                status_text,
                                current,
                                data_size,
                                st.session_state.analysis_start_time
                            )
                        )
                    
                    if df is not None and not df.empty:
                        st.session_state.analysis_data = df
                        st.session_state.show_results = True
                        st.session_state.last_dataset = selected_dataset['filename']
                        st.session_state.last_models = selected_models
                        
                        # ì§„í–‰ ìƒí™© ì»¨í…Œì´ë„ˆ ì œê±°
                        progress_container.empty()
                        status_container.empty()
                        
                        st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
                except Exception as e:
                    st.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)

                # ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼ ìœ„ì¹˜ ë³€ê²½ ë° ë©”ì‹œì§€ ê°œì„ 
            if st.sidebar.button("ë¶„ì„ ìºì‹œ ì´ˆê¸°í™”", type="primary"):
                if clear_analysis_cache():
                    st.sidebar.success("âœ… ëª¨ë“  ë¶„ì„ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.sidebar.error("âŒ ìºì‹œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    # ë©”ì¸ ì˜ì—­ - í¬ë¡¤ë§ ëª¨ë“œì¼ ë•Œ ìƒíƒœ í‘œì‹œ
    if dashboard_mode == "í¬ë¡¤ë§":
        # í¬ë¡¤ë§ ìƒíƒœ ì»¨í…Œì´ë„ˆ ìƒì„±
        status_container = st.empty()
        
        # í¬ë¡¤ë§ ì¤‘ì´ ì•„ë‹ ë•ŒëŠ” ìµœê·¼ ê²°ê³¼ í‘œì‹œ
        if not crawler_status['is_running']:
            with status_container.container():
                st.subheader("ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼")
                results = load_latest_results()
                
                if results:
                    # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
                    st.write("ìµœê·¼ ìˆ˜ì§‘ëœ ë°ì´í„°ì…‹:")
                    result_df = pd.DataFrame([
                        {
                            "í”Œë«í¼": r["platform"],
                            "í‚¤ì›Œë“œ": r["keywords"],
                            "í•­ëª© ìˆ˜": r["items"],
                            "ìˆ˜ì§‘ ì‹œê°„": r["modified"],
                            "íŒŒì¼ëª…": r["filename"]
                        } for r in results
                    ])
                    st.dataframe(result_df, use_container_width=True)
                    
                    # í†µê³„ í‘œì‹œ
                    stats = get_latest_result_stats()
                    if stats:
                        st.subheader("í†µê³„ ìš”ì•½")
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.metric("ì´ ìˆ˜ì§‘ í•­ëª©", stats['total'])
                            
                            # í”Œë«í¼ë³„ í•­ëª© ìˆ˜
                            st.write("í”Œë«í¼ë³„ í•­ëª© ìˆ˜:")
                            platform_data = [{"í”Œë«í¼": p, "í•­ëª© ìˆ˜": c} for p, c in stats['platform_counts'].items()]
                            platform_df = pd.DataFrame(platform_data)
                            st.dataframe(platform_df, use_container_width=True)
                        
                        with col2:
                            # ê°ì„± ë¶„í¬
                            sentiment_data = [{"ê°ì„±": s, "í•­ëª© ìˆ˜": c} for s, c in stats['sentiment_counts'].items()]
                            sentiment_df = pd.DataFrame(sentiment_data)
                            
                            # ì°¨íŠ¸
                            fig, ax = plt.subplots()
                            bars = ax.bar(sentiment_df['ê°ì„±'], sentiment_df['í•­ëª© ìˆ˜'])
                            
                            # ìƒ‰ìƒ ì„¤ì •
                            colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'unknown': 'lightgray'}
                            for i, bar in enumerate(bars):
                                sentiment = sentiment_df.iloc[i]['ê°ì„±']
                                bar.set_color(colors.get(sentiment, 'blue'))
                                
                            plt.title("ê°ì„± ë¶„í¬")
                            st.pyplot(fig)
                        
                        # ìµœê·¼ í¬ë¡¤ë§ ì‹œê°„
                        st.caption(f"ìµœê·¼ í¬ë¡¤ë§: {results[0]['modified'] if results else 'ì—†ìŒ'}")
                else:
                    st.info("ì•„ì§ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
            # ë°ì´í„° ìœ„ì¹˜ ì•ˆë‚´
            st.markdown("---")
            st.markdown("##### ìˆ˜ì§‘ëœ ë°ì´í„° ìœ„ì¹˜")
            st.code("data/raw/*.json")
            st.caption("ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ìœ„ ê²½ë¡œì— JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ëª¨ë“œì—ì„œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            # í¬ë¡¤ë§ ì¤‘ì¼ ë•ŒëŠ” ìƒíƒœ ì •ë³´ í‘œì‹œ
            with status_container.container():
                st.subheader("í¬ë¡¤ë§ ìƒíƒœ")
                
                # ìƒíƒœ ì •ë³´ í‘œì‹œ
                col1, col2 = st.columns(2)
                with col1:
                    st.metric(
                        "ìƒíƒœ",
                        "ì‹¤í–‰ ì¤‘",
                        delta=None
                    )
                with col2:
                    if crawler_status.get('start_time'):
                        st.metric(
                            "ì‹œì‘ ì‹œê°„",
                            datetime.fromtimestamp(crawler_status['start_time']).strftime("%Y-%m-%d %H:%M:%S"),
                            delta=None
                        )
                
                # í”Œë«í¼ë³„ ì§„í–‰ ìƒí™©
                st.subheader("í”Œë«í¼ë³„ ì§„í–‰ ìƒí™©")
                platform_progress = crawler_status.get('platform_progress', {})
                
                for platform, progress in platform_progress.items():
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.text(f"{platform.upper()}")
                        st.progress(progress)
                        st.text(f"{progress*100:.1f}%")
                    with col2:
                        if crawler_status.get('platform_times', {}).get(platform, {}).get('start'):
                            start_time = datetime.strptime(
                                crawler_status['platform_times'][platform]['start'],
                                "%Y-%m-%d %H:%M:%S"
                            )
                            if crawler_status['platform_times'][platform].get('end'):
                                end_time = datetime.strptime(
                                    crawler_status['platform_times'][platform]['end'],
                                    "%Y-%m-%d %H:%M:%S"
                                )
                                duration = end_time - start_time
                                st.text(f"ì†Œìš” ì‹œê°„: {duration}")
                            else:
                                elapsed = datetime.now() - start_time
                                st.text(f"ê²½ê³¼ ì‹œê°„: {elapsed}")
                
                # í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
                st.subheader("í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
                if crawler_status.get('total_items', 0) > 0:
                    st.metric("ì´ ìˆ˜ì§‘ í•­ëª©", crawler_status['total_items'])
                    st.metric("ì²˜ë¦¬ëœ í•­ëª©", crawler_status.get('processed_items', 0))
                
                # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
                if crawler_status.get('error'):
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {crawler_status['error']}")
                
                # ìë™ ìƒˆë¡œê³ ì¹¨ì„ ìœ„í•œ ëŒ€ê¸°
                time.sleep(1)
                st.experimental_rerun()
    elif dashboard_mode == "ë°ì´í„° ë¶„ì„":
        # ë©”ì¸ í™”ë©´ì— ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        # ë¶„ì„ ê²°ê³¼ íŒŒì¼ ëª©ë¡
        analysis_results = datasets['analysis_results']
        
        if not analysis_results:
            st.info("ë¶„ì„ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìƒˆë¡œìš´ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
        else:
            # ë¶„ì„ ê²°ê³¼ ì„ íƒ ì˜µì…˜ êµ¬ì„±
            result_options = []
            for result in analysis_results:
                sentiment_dist = result['sentiment_distribution']
                label = (f"ğŸ“Š {result['filename']} ({result['item_count']}ê°œ) - "
                        f"ë¶„ì„: {result['analysis_time']} - "
                        f"ê°ì„±: {sentiment_dist.get('positive', 'N/A')} ê¸ì •")
                result_options.append((label, result))
            
            # ë¶„ì„ ê²°ê³¼ ì„ íƒ
            selected_label = st.selectbox(
                "ë¶„ì„ ê²°ê³¼ ì„ íƒ",
                options=[opt[0] for opt in result_options],
                help="ìµœì‹  ë¶„ì„ ê²°ê³¼ê°€ ìƒë‹¨ì— í‘œì‹œë©ë‹ˆë‹¤."
            )
            
            # ì„ íƒëœ ê²°ê³¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            selected_result = next(opt[1] for opt in result_options if opt[0] == selected_label)
            
            # ë¶„ì„ ê²°ê³¼ ë°ì´í„° ë¡œë“œ
            try:
                df = pd.read_csv(selected_result['csv_file'])
                
                # ê°ì„± ê°’ ë³€í™˜ (ìˆ«ì -> ë¬¸ìì—´)
                sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}
                df['sentiment'] = df['sentiment'].map(sentiment_map)
                
                # ì‹ ë¢°ë„ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
                df['confidence'] = df['confidence'].apply(lambda x: f"{float(x)*100:.1f}%")
                
                st.session_state.analysis_data = df
                st.session_state.show_results = True
                
                # ê°ì„± ë¶„í¬ ê³„ì‚°
                sentiment_counts = df['sentiment'].value_counts()
                total = len(df)
                sentiment_distribution = {
                    'positive': f"{sentiment_counts.get('positive', 0) / total * 100:.1f}%",
                    'neutral': f"{sentiment_counts.get('neutral', 0) / total * 100:.1f}%",
                    'negative': f"{sentiment_counts.get('negative', 0) / total * 100:.1f}%"
                }
                
                # ë¶„ì„ ì •ë³´ ì—…ë°ì´íŠ¸
                with st.expander("ğŸ“‹ ë¶„ì„ ì •ë³´", expanded=True):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**ë¶„ì„ ì‹œê°„:** {selected_result['analysis_time']}")
                        st.markdown(f"**ì‚¬ìš© ëª¨ë¸:** {', '.join(selected_result['models'])}")
                    with col2:
                        st.markdown(f"**í•­ëª© ìˆ˜:** {len(df)}ê°œ")
                        st.markdown("**ê°ì„± ë¶„í¬:**")
                        st.metric("ê¸ì •", sentiment_distribution['positive'])
                        st.metric("ì¤‘ë¦½", sentiment_distribution['neutral'])
                        st.metric("ë¶€ì •", sentiment_distribution['negative'])
                
            except Exception as e:
                st.error(f"ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                logger.error(f"ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            
            if st.session_state.analysis_data is not None and isinstance(st.session_state.analysis_data, pd.DataFrame) and not st.session_state.analysis_data.empty:
                df = st.session_state.analysis_data
                
                st.markdown("### ğŸ“Š ë¬¸ì¥ ê°ì„± ë¶„í¬", help="ì´ ë°ì´í„°ëŠ” ì§€ì—­ë³„ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì§€ë„ì— ì‹œê°í™”í•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°ì†ì§€ì ë„ì™€ ì—°ê³„í•˜ì—¬ íŠ¹ì • ì§€ì—­ì˜ ê¸ì •/ë¶€ì • í”¼ë“œë°±ì„ ì§€ë„ìƒì— í‘œì‹œí•˜ë©´ ì§€ì—­ ê°œë°œ ë° ì •ì±… ìˆ˜ë¦½ì— ë”ìš± íš¨ê³¼ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # ì§€ì—­ í‚¤ì›Œë“œ ê´€ë ¨ ë¬¸ì¥ í•„í„°ë§ ë° ì •ë ¬
                region_keywords = ['ë¶€ì•ˆ', 'ë³€ì‚°', 'ë‚´ì†Œ', 'ì±„ì„ê°•', 'ê³ ì‚¬í¬', 'ê²©í¬', 'ìœ„ë„', 'ê³„í™”', 'ì¤„í¬']
                
                # ì§€ì—­ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
                df['has_region_keyword'] = df['content'].apply(
                    lambda x: any(keyword in x for keyword in region_keywords)
                )
                
                # ì‹ ë¢°ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (confidence ì»¬ëŸ¼ì—ì„œ % ì œê±°í•˜ê³  floatë¡œ ë³€í™˜)
                df['confidence_value'] = df['confidence'].str.rstrip('%').astype(float)
                
                # ì§€ì—­ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¤‘ ì‹ ë¢°ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                region_df = df[df['has_region_keyword']].sort_values(
                    by=['confidence_value', 'has_region_keyword'],
                    ascending=[False, False]
                ).head(50)
                
                # ê°ì„±ë³„ë¡œ ìƒ‰ìƒ ì§€ì •
                sentiment_colors = {
                    'positive': 'ğŸŸ¢',  # ê¸ì •: ì´ˆë¡ìƒ‰
                    'neutral': 'âšª',   # ì¤‘ë¦½: í°ìƒ‰
                    'negative': 'ğŸ”´'   # ë¶€ì •: ë¹¨ê°„ìƒ‰
                }
                
                # ê°ì„±ë³„ í†µê³„
                sentiment_stats = region_df['sentiment'].value_counts()
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ê¸ì •", f"{sentiment_stats.get('positive', 0)}ê°œ")
                with col2:
                    st.metric("ì¤‘ë¦½", f"{sentiment_stats.get('neutral', 0)}ê°œ")
                with col3:
                    st.metric("ë¶€ì •", f"{sentiment_stats.get('negative', 0)}ê°œ")
                
                # ê°ì„±ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
                positive_df = region_df[region_df['sentiment'] == 'positive']
                negative_df = region_df[region_df['sentiment'] == 'negative']
                neutral_df = region_df[region_df['sentiment'] == 'neutral']
                
                # ê¸ì • ë¬¸ì¥ í‘œì‹œ (ìƒìœ„ 5ê°œ)
                if not positive_df.empty:
                    st.markdown("### ğŸŸ¢ ê¸ì • ë¬¸ì¥")
                    # ìƒìœ„ 5ê°œ ë¬¸ì¥ë§Œ í‘œì‹œ
                    for _, row in positive_df.head(5).iterrows():
                        with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                            if 'platform' in row:
                                st.caption(f"ì¶œì²˜: {row['platform']}")
                            if 'published_date' in row:
                                st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                            
                            # ì§€ì—­ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¶€ë¶„ ì°¾ê¸°
                            content = row['content']
                            for keyword in region_keywords:
                                if keyword in content:
                                    # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ë’¤ 50ì)
                                    idx = content.find(keyword)
                                    start = max(0, idx - 50)
                                    end = min(len(content), idx + len(keyword) + 50)
                                    highlight = content[start:end]
                                    if start > 0:
                                        highlight = "..." + highlight
                                    if end < len(content):
                                        highlight = highlight + "..."
                                    st.markdown(highlight)
                                    break
                            
                            st.markdown("**ì „ì²´ ë‚´ìš©:**")
                            st.markdown(content)
                    
                    # 5ê°œ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë” ë³´ê¸° ë²„íŠ¼ í‘œì‹œ
                    if len(positive_df) > 5:
                        col1, col2 = st.columns([1, 1])
                        with col1:
                            if st.button(f"ğŸŸ¢ ê¸ì • ë¬¸ì¥ ë” ë³´ê¸° ({len(positive_df)-5}ê°œ ë” ìˆìŒ)", key="more_positive"):
                                st.session_state.show_all_positive = True
                                st.rerun()
                        with col2:
                            if st.session_state.get('show_all_positive', False):
                                if st.button("ë‹«ê¸°", key="close_positive_top"):
                                    st.session_state.show_all_positive = False
                                    st.rerun()
                    
                    # ê¸ì • ë¬¸ì¥ ë” ë³´ê¸° ì„¹ì…˜
                    if st.session_state.get('show_all_positive', False):
                        st.markdown("#### ğŸŸ¢ ê¸ì • ë¬¸ì¥ ì „ì²´ ë³´ê¸°")
                        for _, row in positive_df.iterrows():
                            with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                                if 'platform' in row:
                                    st.caption(f"ì¶œì²˜: {row['platform']}")
                                if 'published_date' in row:
                                    st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                                st.markdown(row['content'])
                        if st.button("ë‹«ê¸°", key="close_positive_bottom"):
                            st.session_state.show_all_positive = False
                            st.rerun()
                
                # ë¶€ì • ë¬¸ì¥ í‘œì‹œ (ìƒìœ„ 5ê°œ)
                if not negative_df.empty:
                    st.markdown("### ğŸ”´ ë¶€ì • ë¬¸ì¥")
                    # ìƒìœ„ 5ê°œ ë¬¸ì¥ë§Œ í‘œì‹œ
                    for _, row in negative_df.head(5).iterrows():
                        with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                            if 'platform' in row:
                                st.caption(f"ì¶œì²˜: {row['platform']}")
                            if 'published_date' in row:
                                st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                            
                            # ì§€ì—­ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¶€ë¶„ ì°¾ê¸°
                            content = row['content']
                            for keyword in region_keywords:
                                if keyword in content:
                                    # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ë’¤ 50ì)
                                    idx = content.find(keyword)
                                    start = max(0, idx - 50)
                                    end = min(len(content), idx + len(keyword) + 50)
                                    highlight = content[start:end]
                                    if start > 0:
                                        highlight = "..." + highlight
                                    if end < len(content):
                                        highlight = highlight + "..."
                                    st.markdown(highlight)
                                    break
                            
                            st.markdown("**ì „ì²´ ë‚´ìš©:**")
                            st.markdown(content)
                    
                    # 5ê°œ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë” ë³´ê¸° ë²„íŠ¼ í‘œì‹œ
                    if len(negative_df) > 5:
                        col1, col2 = st.columns([1, 1])
                        with col1:
                            if st.button(f"ğŸ”´ ë¶€ì • ë¬¸ì¥ ë” ë³´ê¸° ({len(negative_df)-5}ê°œ ë” ìˆìŒ)", key="more_negative"):
                                st.session_state.show_all_negative = True
                                st.rerun()
                        with col2:
                            if st.session_state.get('show_all_negative', False):
                                if st.button("ë‹«ê¸°", key="close_negative_top"):
                                    st.session_state.show_all_negative = False
                                    st.rerun()
                    
                    # ë¶€ì • ë¬¸ì¥ ë” ë³´ê¸° ì„¹ì…˜
                    if st.session_state.get('show_all_negative', False):
                        st.markdown("#### ğŸ”´ ë¶€ì • ë¬¸ì¥ ì „ì²´ ë³´ê¸°")
                        for _, row in negative_df.iterrows():
                            with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                                if 'platform' in row:
                                    st.caption(f"ì¶œì²˜: {row['platform']}")
                                if 'published_date' in row:
                                    st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                                st.markdown(row['content'])
                        if st.button("ë‹«ê¸°", key="close_negative_bottom"):
                            st.session_state.show_all_negative = False
                            st.rerun()
                
                # ì¤‘ë¦½ ë¬¸ì¥ í‘œì‹œ (ìƒìœ„ 5ê°œ)
                if not neutral_df.empty:
                    st.markdown("### âšª ì¤‘ë¦½ ë¬¸ì¥")
                    # ìƒìœ„ 5ê°œ ë¬¸ì¥ë§Œ í‘œì‹œ
                    for _, row in neutral_df.head(5).iterrows():
                        with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                            if 'platform' in row:
                                st.caption(f"ì¶œì²˜: {row['platform']}")
                            if 'published_date' in row:
                                st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                            
                            # ì§€ì—­ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¶€ë¶„ ì°¾ê¸°
                            content = row['content']
                            for keyword in region_keywords:
                                if keyword in content:
                                    # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ë’¤ 50ì)
                                    idx = content.find(keyword)
                                    start = max(0, idx - 50)
                                    end = min(len(content), idx + len(keyword) + 50)
                                    highlight = content[start:end]
                                    if start > 0:
                                        highlight = "..." + highlight
                                    if end < len(content):
                                        highlight = highlight + "..."
                                    st.markdown(highlight)
                                    break
                            
                            st.markdown("**ì „ì²´ ë‚´ìš©:**")
                            st.markdown(content)
                    
                    # 5ê°œ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë” ë³´ê¸° ë²„íŠ¼ í‘œì‹œ
                    if len(neutral_df) > 5:
                        col1, col2 = st.columns([1, 1])
                        with col1:
                            if st.button(f"âšª ì¤‘ë¦½ ë¬¸ì¥ ë” ë³´ê¸° ({len(neutral_df)-5}ê°œ ë” ìˆìŒ)", key="more_neutral"):
                                st.session_state.show_all_neutral = True
                                st.rerun()
                        with col2:
                            if st.session_state.get('show_all_neutral', False):
                                if st.button("ë‹«ê¸°", key="close_neutral_top"):
                                    st.session_state.show_all_neutral = False
                                    st.rerun()
                    
                    # ì¤‘ë¦½ ë¬¸ì¥ ë” ë³´ê¸° ì„¹ì…˜
                    if st.session_state.get('show_all_neutral', False):
                        st.markdown("#### âšª ì¤‘ë¦½ ë¬¸ì¥ ì „ì²´ ë³´ê¸°")
                        for _, row in neutral_df.iterrows():
                            with st.expander(f"**{row['title']}** (ì‹ ë¢°ë„: {row['confidence']})"):
                                if 'platform' in row:
                                    st.caption(f"ì¶œì²˜: {row['platform']}")
                                if 'published_date' in row:
                                    st.caption(f"ì‘ì„±ì¼: {row['published_date']}")
                                st.markdown(row['content'])
                        if st.button("ë‹«ê¸°", key="close_neutral_bottom"):
                            st.session_state.show_all_neutral = False
                            st.rerun()
                
                # ì§€ì—­ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš° ì•ˆë‚´
                if len(region_df) == 0:
                    st.info("ì§€ì—­ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
                st.markdown("### ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    csv,
                    file_name=f"sentiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime='text/csv',
                    help="ë¶„ì„ëœ ì „ì²´ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                )
        
        # ê°ì„± ë¶„í¬ ì‹œê°í™”
                st.markdown("### ğŸ“Š ê°ì„± ë¶„í¬")
                if st.session_state.analysis_data is not None and isinstance(st.session_state.analysis_data, pd.DataFrame) and not st.session_state.analysis_data.empty:
                    df = st.session_state.analysis_data
                    if 'sentiment' in df.columns:
                        col1, col2 = st.columns(2)
                        with col1:
                            # ë§‰ëŒ€ ê·¸ë˜í”„
                            sentiment_counts = df['sentiment'].value_counts()
                            fig, ax = plt.subplots(figsize=(10, 6))
                            bars = ax.bar(sentiment_counts.index, sentiment_counts.values)
                            
                            # ìƒ‰ìƒ ì„¤ì •
                            colors = {
                                'positive': '#2ecc71',  # ê¸ì •: ì´ˆë¡ìƒ‰
                                'neutral': '#95a5a6',   # ì¤‘ë¦½: íšŒìƒ‰
                                'negative': '#e74c3c'   # ë¶€ì •: ë¹¨ê°„ìƒ‰
                            }
                            for bar, sentiment in zip(bars, sentiment_counts.index):
                                bar.set_color(colors.get(sentiment, '#3498db'))
                            
                            plt.title("ê°ì„± ë¶„í¬")
                            plt.xticks(rotation=45)
                            st.pyplot(fig)
                            
                        with col2:
                            # íŒŒì´ ì°¨íŠ¸
                            fig, ax = plt.subplots(figsize=(10, 6))
                            sentiment_counts.plot(
                                kind='pie',
                                autopct='%1.1f%%',
                                ax=ax,
                                colors=[colors.get(s, '#3498db') for s in sentiment_counts.index]
                            )
                            plt.title("ê°ì„± ë¶„í¬ (ë¹„ìœ¨)")
                            st.pyplot(fig)
                    else:
                        st.info("ê°ì„± ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹œê³„ì—´ íŠ¸ë Œë“œ
                st.markdown("### ğŸ“ˆ ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ")
                if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
                    df = pd.DataFrame(st.session_state.analysis_data)
                    
                    # ë‚ ì§œ í˜•ì‹ ì •ê·œí™” ë° ë³€í™˜
                    valid_dates = []
                    for idx, row in df.iterrows():
                        try:
                            date_str = row['published_date']
                            if date_str.isdigit() and len(date_str) == 8:
                                df.at[idx, 'date'] = pd.to_datetime(date_str, format='%Y%m%d')
                                valid_dates.append(idx)
                        except:
                            pass
                    
                    # ìœ íš¨í•œ ë‚ ì§œë§Œ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                    df_valid = df.loc[valid_dates]
                    
                    if not df_valid.empty:
                        daily_sentiment = df_valid.groupby(['date', 'sentiment']).size().unstack(fill_value=0)
                        fig, ax = plt.subplots(figsize=(12, 6))
                        daily_sentiment.plot(kind='line', ax=ax)
                        plt.title("ì¼ë³„ ê°ì„± íŠ¸ë Œë“œ")
                        st.pyplot(fig)
                    else:
                        st.warning("ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ í‘œì‹œí•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì›Œë“œí´ë¼ìš°ë“œ
                st.markdown("### â˜ï¸ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
                if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
                    df = pd.DataFrame(st.session_state.analysis_data)
                    col1, col2 = st.columns(2)
                    with col1:
                        # ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ
                        text = ' '.join(df['content'])
                        if text.strip():
                            wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=korean_font_path).generate(text)
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.imshow(wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            plt.title("ì „ì²´ ì»¨í…ì¸  ì›Œë“œí´ë¼ìš°ë“œ")
                            st.pyplot(fig)
                        else:
                            st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    with col2:
                        # ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ
                        positive_text = ' '.join(df[df['sentiment'] == 'positive']['content'])
                        if positive_text.strip():
                            wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=korean_font_path).generate(positive_text)
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.imshow(wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            plt.title("ê¸ì • ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ")
                            st.pyplot(fig)
                        else:
                            st.info("ê¸ì • ê°ì„±ì˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # GPT ë¦¬í¬íŠ¸ ìƒì„±
                st.markdown("### ğŸ“ ì •ì±… ì œì•ˆ ë¦¬í¬íŠ¸")
                if st.session_state.analysis_data is not None and len(st.session_state.analysis_data) > 0:
                    try:
                        df = pd.DataFrame(st.session_state.analysis_data)
                        report_generator = GPTReportGenerator(api_key=os.getenv("OPENAI_API_KEY"))
                        report = report_generator.generate_report(df)
                        st.text(report)
                        
                        # PDF ë¦¬í¬íŠ¸ ì €ì¥
                        pdf_generator = PDFReportGenerator()
                        pdf_path = pdf_generator.generate_pdf(report)
                        if pdf_path:
                            with open(pdf_path, "rb") as f:
                                st.download_button(
                                    "ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                                    f,
                                    file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                                    mime='application/pdf'
                                )
                    except Exception as e:
                        st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                else:
                    st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 
    